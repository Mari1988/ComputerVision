{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load training data set from CSV file\n",
    "training_data_df = pd.read_csv(\"./data/sales_data_training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load testing data set from CSV file\n",
    "test_data_df = pd.read_csv(\"./data/sales_data_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     count        mean           std       min       25%  \\\n",
      "critic_rating       1000.0       3.660      0.834024      2.00      3.00   \n",
      "is_action           1000.0       0.466      0.499092      0.00      0.00   \n",
      "is_exclusive_to_us  1000.0       0.267      0.442614      0.00      0.00   \n",
      "is_portable         1000.0       0.243      0.429110      0.00      0.00   \n",
      "is_role_playing     1000.0       0.347      0.476254      0.00      0.00   \n",
      "is_sequel           1000.0       0.746      0.435515      0.00      0.00   \n",
      "is_sports           1000.0       0.187      0.390107      0.00      0.00   \n",
      "suitable_for_kids   1000.0       0.270      0.444182      0.00      0.00   \n",
      "total_earnings      1000.0  110705.229  44970.558163  31355.00  78830.25   \n",
      "unit_price          1000.0      54.170      8.036927     39.99     49.99   \n",
      "\n",
      "                          50%        75%        max  \n",
      "critic_rating            3.75       4.50       5.00  \n",
      "is_action                0.00       1.00       1.00  \n",
      "is_exclusive_to_us       0.00       1.00       1.00  \n",
      "is_portable              0.00       0.00       1.00  \n",
      "is_role_playing          0.00       1.00       1.00  \n",
      "is_sequel                1.00       1.00       1.00  \n",
      "is_sports                0.00       0.00       1.00  \n",
      "suitable_for_kids        0.00       1.00       1.00  \n",
      "total_earnings      104335.50  133271.00  301860.00  \n",
      "unit_price              59.99      59.99      59.99  \n"
     ]
    }
   ],
   "source": [
    "print(training_data_df.describe().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data needs to be scaled to a small range like 0 to 1 for the neural\n",
    "# network to work well.\n",
    "scaler = MinMaxScaler(feature_range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.37471396  0.19242528  0.11485185  0.14245208  0.48068243  0.13972015\n",
      "  0.11338792  0.44906748  0.06127428  0.2066801   0.47775457  0.13232657\n",
      "  0.17925362  0.16335742  0.23946692  0.31206817  0.22852812  0.29274505\n",
      "  0.34805641  0.42799209  0.22483133  0.12802351  0.40164507  0.29634203\n",
      "  0.24860908  0.2892331   0.06283433  0.29536607  0.26932589  0.10460805\n",
      "  0.46514852  0.28864901  0.17730171  0.19321639  0.24883459  0.41745254\n",
      "  0.27342563  0.5790429   0.29860077  0.17740892  0.2085248   0.01365964\n",
      "  0.2546903   0.28747713  0.45726327  0.12294043  0.43984769  0.50644535\n",
      "  0.24063881  0.16276963  0.27108187  0.14089204  0.73536534  0.25351842\n",
      "  0.24181069  0.3442672   0.15984178  0.2474409   0.323543    0.21839153\n",
      "  0.09288553  0.0370751   0.18187095  0.11885917  0.15984178  0.13894013\n",
      "  0.55576422  0.29157687  0.33841149  0.71370215  0.44438365  0.316752\n",
      "  0.63583298  0.10265614  0.21019574  0.38759357  0.46131495  0.47248665\n",
      "  0.12939872  0.0280993   0.17623704  0.3308035   0.18616292  0.14753886\n",
      "  0.11942478  0.17272509  0.16160145  0.58138297  0.42564832  0.36415963\n",
      "  0.2751816   0.4759986   0.12647086  0.20901277  0.15729839  0.21136393\n",
      "  0.11942478  0.67857156  0.26522985  0.1509547   0.38173786  0.29274505\n",
      "  0.19241049  0.07298202  0.40164507  0.20364503  0.12529898  0.33489954\n",
      "  0.29274505  0.22755217  0.14362396  0.2392599   0.          0.1919373\n",
      "  0.37061422  0.2066801   0.1509547   0.23537088  0.18772666  0.34836694\n",
      "  0.05562929  0.05045008  0.18443282  0.34661097  0.21270217  0.11825659\n",
      "  0.30914031  0.30122179  0.13875899  0.1651134   0.20609601  0.44648343\n",
      "  0.5445001   0.34891407  0.21114212  0.24181069  0.50234561  0.25761816\n",
      "  0.40004066  0.08607235  0.28981719  0.32845973  0.12178703  0.09240864\n",
      "  0.11903662  0.48478217  0.0441027   0.3134175   0.20608492  0.21621781\n",
      "  0.42857618  0.38349384  0.33342082  0.19731243  0.03630986  0.43711576\n",
      "  0.25351842  0.01795161  0.07202085  0.24169978  0.16042957  0.30796843\n",
      "  0.12881462  0.31323635  0.31440824  0.15884364  0.07805771  0.31323635\n",
      "  0.12879244  0.15388255  0.42330826  0.4414558   0.42447644  0.17565295\n",
      "  0.29216096  0.53337646  0.25682335  0.07454206  0.31323635  0.6615959\n",
      "  0.37588215  0.17445519  0.2962607   0.4619471   0.24782906  0.30914031\n",
      "  0.50346574  0.16160145  0.1305706   0.4549232   0.80971886  0.09719599\n",
      "  0.82025841  0.43852794  0.16569749  0.66393597  0.489466    0.24532264\n",
      "  0.08724423  0.29157687  0.05728175  0.25703037  0.12295891  0.48185431\n",
      "  0.08625349  0.66627974  0.28806122  0.37998189  0.25836861  0.48653814\n",
      "  0.39871721  0.25879004  0.17583778  0.27635349  0.26581394  0.66218\n",
      "  0.42799209  0.33489954  0.10021626  0.21660598  0.23127114  0.24782906\n",
      "  0.29157687  0.34171642  0.2751816   0.17827767  0.1822628   0.20364503\n",
      "  0.22267241  0.14206392  0.54077004  0.42623242  0.07728508  0.42916027\n",
      "  0.27108187  0.25000647  0.09600932  0.49393542  0.29536607  0.43984769\n",
      "  0.58028872  0.23127114  0.27342563  0.27635349  0.16862535  0.45257943\n",
      "  0.17623704  0.54567198  0.23653907  0.12471489  0.42916027  0.19486516\n",
      "  0.25682335  0.30094453  0.33607142  0.19087263  0.34075525  0.1822628\n",
      "  0.32237482  0.58138297  0.29040498  0.15534648  0.16743129  0.24239478\n",
      "  0.17096911  0.21019574  0.23712686  0.19438827  0.35480675  0.72775365\n",
      "  0.0480139   0.18304283  0.4417996   0.04362211  0.29977265  0.18502061\n",
      "  0.80093898  0.35480675  0.19497237  0.2546903   0.43326001  0.18900945\n",
      "  0.25293433  0.2660949   0.47482671  0.33255947  0.38232195  0.29274505\n",
      "  0.17290993  0.23127114  0.11162086  0.27097466  0.12265947  0.73360936\n",
      "  0.27400972  0.18209275  0.49356204  0.27108187  0.23244302  0.57143121\n",
      "  0.54976803  0.34017116  0.32122512  0.74824495  0.2201475   0.14217482\n",
      "  0.43831722  0.23712686  0.57931277  0.54215634  0.42096449  0.23487181\n",
      "  0.21231401  0.22541543  0.0870446   0.17038132  0.0480139   0.15415981\n",
      "  0.42564832  0.4137003   1.          0.04917469  0.29742888  0.25059056\n",
      "  0.30562466  0.58314264  0.18382285  0.11680376  0.36100996  0.37412617\n",
      "  0.123543    0.21487958  0.21253581  0.20024029  0.91100719  0.3103085\n",
      "  0.64988448  0.35773461  0.46563649  0.21663555  0.123543    0.37037763\n",
      "  0.11006081  0.17016321  0.53396056  0.43208813  0.3441563   0.19555646\n",
      "  0.38817767  0.51873718  0.32963161  0.17778969  0.23633205  0.65005453\n",
      "  0.23829874  0.22950408  0.26853478  0.25351842  0.06907821  0.33372766\n",
      "  0.18850668  0.17857711  0.38642169  0.16508752  0.18705754  0.29860077\n",
      "  0.30562466  0.42330826  0.47307074  0.21504593  0.29157687  0.64637253\n",
      "  0.32902904  0.01678342  0.20510896  0.5860668   0.16042957  0.29391693\n",
      "  0.13466664  0.75936859  0.33021571  0.53101052  0.1426628   0.13894013\n",
      "  0.13875899  0.22131938  0.57553095  0.27166965  0.20843607  0.38349384\n",
      "  0.23946692  0.18382285  0.27049777  0.66452376  0.27983217  0.10927709\n",
      "  0.28334412  0.00117188  0.29536607  0.15778636  0.11359125  0.36183065\n",
      "  0.13875899  0.57143121  0.06801723  0.67037578  0.10890741  0.16686937\n",
      "  0.04566274  0.22852812  0.10314412  0.32159479  0.36530563  0.13074435\n",
      "  0.41682039  0.15583446  0.18443282  0.12939872  0.38642169  0.64461655\n",
      "  0.58723868  0.1572836   0.27342563  0.29274505  0.71428624  0.38349384\n",
      "  0.0544574   0.5790429   0.09093732  0.23487181  0.08625349  0.34895104\n",
      "  0.30914031  0.43294948  0.42623242  0.21956341  0.37412617  0.26267906\n",
      "  0.58138297  0.17506516  0.28073049  0.37061422  0.36768636  0.19380048\n",
      "  0.3442672   0.6615959   0.06166245  0.53806029  0.07454206  0.37529805\n",
      "  0.0480139   0.11825659  0.47365853  0.58489862  0.3372433   0.20024029\n",
      "  0.39578936  0.24853145  0.9724811   0.16628528  0.12881462  0.06606532\n",
      "  0.11630469  0.12705865  0.50034195  0.09134397  0.09678934  0.27225375\n",
      "  0.19388921  0.29391693  0.28256409  0.15583446  0.54274413  0.08898542\n",
      "  0.11143602  0.07826103  0.29508882  0.23244302  0.16821131  0.18970444\n",
      "  0.38349384  0.16703943  0.28162141  0.23712686  0.15867359  0.30445648\n",
      "  0.29098908  0.37646994  0.08937358  0.10597956  0.16547938  0.49005009\n",
      "  0.18577475  0.31323635  0.24822092  0.19146042  0.11396092  0.48185431\n",
      "  0.27225375  0.47482671  0.17389327  0.0953365   0.30504057  0.89051589\n",
      "  0.09240864  0.10147317  0.27400972  0.34227833  0.27108187  0.26756992\n",
      "  0.40808488  0.29536607  0.38466572  0.489466    0.30914031  0.65222824\n",
      "  0.34485499  0.20364503  0.30679655  0.25926323  0.63700486  0.38232195\n",
      "  0.38708711  0.22190348  0.28279329  0.22519362  0.29508882  0.23771095\n",
      "  0.49063788  0.27108187  0.20843607  0.48863422  0.23829874  0.22364836\n",
      "  0.34318035  0.15691762  0.28139221  0.0140515   0.60712002  0.23653907\n",
      "  0.25975121  0.1899854   0.32081108  0.37881     0.61007375  0.43267592\n",
      "  0.75761261  0.18970444  0.40316075  0.15494353  0.27400972  0.13876638\n",
      "  0.07649396  0.36066246  0.12471489  0.234199    0.38115377  0.31206817\n",
      "  0.49370622  0.15337979  0.41745254  0.31089629  0.47951055  0.23478309\n",
      "  0.46604684  0.61007375  0.08724423  0.31382414  0.27585072  0.16042957\n",
      "  0.23244302  0.14401582  0.21429179  0.37881     0.42974806  0.33841149\n",
      "  0.43677196  0.23438384  0.26815771  0.1510619   0.29684479  0.82903828\n",
      "  0.26267906  0.07064195  0.28981719  0.29040498  0.14637807  0.19438827\n",
      "  0.234199    0.25586218  0.1202085   0.23068705  0.29274505  0.10888893\n",
      "  0.72306981  0.06829818  0.2962607   0.32512893  0.48368792  0.13854827\n",
      "  0.2751816   0.24707861  0.19162677  0.39813312  0.15515795  0.61592947\n",
      "  0.1943624   0.18772666  0.10810891  0.33146892  0.1822628   0.22131938\n",
      "  0.07181013  0.10654517  0.2271714   0.06556626  0.30914031  0.21487958\n",
      "  0.14022292  0.34266649  0.13095137  0.24392525  0.28373598  0.19146042\n",
      "  0.68091533  0.48009833  0.08664165  0.20375224  0.34953883  0.44906748\n",
      "  0.33372766  0.80445093  0.30445648  0.33372766  0.9033955   0.34171642\n",
      "  0.19848432  0.22131938  0.29508882  0.32845973  0.56440731  0.29508882\n",
      "  0.10069315  0.34735033  0.4274043   0.66042402  0.57318719  0.47658638\n",
      "  0.53864439  0.12176854  0.14089204  0.26951073  0.39754533  0.71370215\n",
      "  0.81557457  0.3378274   0.62640247  0.33489954  0.26522985  0.14401582\n",
      "  0.30094453  0.17330918  0.65808026  0.23946692  0.24938911  0.30211272\n",
      "  0.29040498  0.24532264  0.14284394  0.81557457  0.16394152  0.23068705\n",
      "  0.18655478  0.18970444  0.18655478  0.22462431  0.15299163  0.2066801\n",
      "  0.30366167  0.28981719  0.29508882  0.40105728  0.08655663  0.32435999\n",
      "  0.19475056  0.14314708  0.37588215  0.38232195  0.47658638  0.31206817\n",
      "  0.45344079  0.64956655  0.06264949  0.30902941  0.61241382  0.29801667\n",
      "  0.34318035  0.18187095  0.32744681  0.111229    0.20316815  0.33489954\n",
      "  0.27983217  0.49063788  0.13174248  0.17506516  0.20551191  0.23478309\n",
      "  0.48829412  0.27166965  0.41862442  0.42514186  0.11289995  0.41803664\n",
      "  0.20551191  0.16949409  0.50234561  0.42974806  0.41862442  0.43267592\n",
      "  0.23771095  0.11769099  0.72482579  0.63981072  0.11631578  0.23946692\n",
      "  0.16042957  0.3027005   0.2915399   0.19670246  0.27975453  0.11476313\n",
      "  0.13191623  0.16335742  0.09368404  0.23829874  0.80854698  0.29860077\n",
      "  0.27635349  0.33781261  0.29742888  0.3606218   0.34485499  0.24883459\n",
      "  0.11708471  0.49766178  0.31733609  0.46319661  0.15437053  0.23537088\n",
      "  0.80620691  0.20364503  0.28981719  0.23946692  0.30562466  0.38115377\n",
      "  0.04020628  0.24005471  0.30738434  0.15533169  0.32319181  0.27049777\n",
      "  0.13894013  0.48271196  0.10069315  0.26932589  0.34777915  0.38208536\n",
      "  0.54098815  0.47775457  0.13174248  0.1791649   0.4880797   0.33878856\n",
      "  0.31323635  0.2066801   0.15925768  0.60538992  0.18608159  0.26932589\n",
      "  0.27195061  0.23653907  0.15494353  0.10773553  0.3512948   0.27986544\n",
      "  0.28902608  0.27049777  0.2577993   0.30738434  0.33138759  0.58224062\n",
      "  0.14754995  0.35398606  0.19438827  0.16569749  0.43033216  0.39379679\n",
      "  0.20434003  0.38115377  0.33138759  0.18033678  0.39403338  0.1446221\n",
      "  0.37646994  0.15515795  0.3103085   0.17799301  0.03630986  0.20434003\n",
      "  0.22541543  0.27049777  0.20258406  0.10771705  0.18677658  0.29274505\n",
      "  0.48515185  0.35012292  0.1662594   0.18382285  0.22541543  0.35944992\n",
      "  0.33138759  0.47248665  0.29860077  0.19438827  0.47482671  0.29634203\n",
      "  0.38700579  0.27810946  0.57143121  0.42096449  0.21429179  0.66276409\n",
      "  0.18033678  0.48563982  0.12647086  0.20492412  0.27342563  0.33255947\n",
      "  0.56850335  0.2201475   0.31323635  0.37049962  0.75761261  0.40164507\n",
      "  0.31488143  0.24181069  0.42623242  0.18794477  0.39106486  0.14206392\n",
      "  0.27007634  0.06986192  0.43091625  0.30914031  0.39052143  0.15047411\n",
      "  0.22755217  0.27048668  0.2892331   0.10655626  0.32963161  0.18850668\n",
      "  0.15884364  0.37763812  0.52810484  0.17290993  0.13095137  0.23829874\n",
      "  0.05229848  0.28396518  0.26463097  0.16743129  0.17778969  0.29661559\n",
      "  0.31323635  0.19876527  0.0825604   0.33607142  0.31261529  0.88700024\n",
      "  0.31323635  0.35187889  0.15485851  0.31964289  0.34017116  0.41394059\n",
      "  0.12178703  0.21604776  0.12764274  0.05142604  0.19047337  0.14071089\n",
      "  0.25290475  0.80445093  0.60565609  0.37763812  0.51346925  0.06127428\n",
      "  0.28279329  0.32081108  0.23537088  0.07181013  0.11903662  0.27342563\n",
      "  0.32315484  0.2384614   0.42709377  0.17875085  0.23127114  0.2577993\n",
      "  0.13191623  0.20657289  0.25000647  0.08859356  0.20434003  0.31558012\n",
      "  0.2546903   0.05914124  0.25937413  0.01873163  0.11825659  0.31558012\n",
      "  0.57377498  0.4344319   0.16686937  0.2558474   0.11289995  0.07181013\n",
      "  0.73360936  0.25234654  0.35597863  0.16452561  0.37529805  0.81398126\n",
      "  0.19486516  0.51171328  0.1202085   0.42974806  0.09288553  0.15415981\n",
      "  0.19632909  0.06986192  0.12764274  0.21231401  0.14022292  0.19096135\n",
      "  0.28981719  0.27195061  0.26815771  0.23650949  0.11162086  0.15494353\n",
      "  0.3812351   0.37646994  0.30024584  0.3372433   0.42096449  0.48302619\n",
      "  0.07766585  0.38349384  0.23244302  0.47775457  0.40398514  0.29391693\n",
      "  0.39871721  0.38466572  0.16851814  0.34192714  0.15337979  0.41050628\n",
      "  0.16276963  0.42213637  0.24275707  0.27108187  0.38349384  0.24415076\n",
      "  0.0294745   0.17330918  0.33841149  0.18384873  0.44101957  0.26932589\n",
      "  0.19731243  0.57669914  0.54742796  0.38115377  0.3442672   0.3103085\n",
      "  0.22011793  0.2763387   0.34192714  0.234199    0.16979723  0.2751816\n",
      "  0.20316815  0.61007375  0.24626902  0.21633242]\n"
     ]
    }
   ],
   "source": [
    "# Scale both the training inputs and outputs\n",
    "scaled_training = scaler.fit_transform(training_data_df) #output\n",
    "scaled_testing = scaler.transform(test_data_df) #transform based training data's min/max\n",
    "print(scaled_training[:,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.553333</td>\n",
       "      <td>0.278008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.466000</td>\n",
       "      <td>0.499092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>0.442614</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.243000</td>\n",
       "      <td>0.429110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.347000</td>\n",
       "      <td>0.476254</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.435515</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.187000</td>\n",
       "      <td>0.390107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.444182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.293341</td>\n",
       "      <td>0.166247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.175506</td>\n",
       "      <td>0.269794</td>\n",
       "      <td>0.376762</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.709000</td>\n",
       "      <td>0.401846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count      mean       std  min       25%       50%       75%  max\n",
       "0  1000.0  0.553333  0.278008  0.0  0.333333  0.583333  0.833333  1.0\n",
       "1  1000.0  0.466000  0.499092  0.0  0.000000  0.000000  1.000000  1.0\n",
       "2  1000.0  0.267000  0.442614  0.0  0.000000  0.000000  1.000000  1.0\n",
       "3  1000.0  0.243000  0.429110  0.0  0.000000  0.000000  0.000000  1.0\n",
       "4  1000.0  0.347000  0.476254  0.0  0.000000  0.000000  1.000000  1.0\n",
       "5  1000.0  0.746000  0.435515  0.0  0.000000  1.000000  1.000000  1.0\n",
       "6  1000.0  0.187000  0.390107  0.0  0.000000  0.000000  0.000000  1.0\n",
       "7  1000.0  0.270000  0.444182  0.0  0.000000  0.000000  1.000000  1.0\n",
       "8  1000.0  0.293341  0.166247  0.0  0.175506  0.269794  0.376762  1.0\n",
       "9  1000.0  0.709000  0.401846  0.0  0.500000  1.000000  1.000000  1.0"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=scaled_training).describe().transpose() #notice the reduction in standard deviation for the continuous attributes like total_earnings\n",
    "#after scaling all the attribute's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>400.0</td>\n",
       "      <td>0.545000</td>\n",
       "      <td>0.271761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>400.0</td>\n",
       "      <td>0.487500</td>\n",
       "      <td>0.500470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>400.0</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.454330</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>400.0</td>\n",
       "      <td>0.272500</td>\n",
       "      <td>0.445803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>400.0</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.467060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>400.0</td>\n",
       "      <td>0.757500</td>\n",
       "      <td>0.429132</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>400.0</td>\n",
       "      <td>0.192500</td>\n",
       "      <td>0.394757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>400.0</td>\n",
       "      <td>0.297500</td>\n",
       "      <td>0.457731</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>400.0</td>\n",
       "      <td>0.304486</td>\n",
       "      <td>0.173502</td>\n",
       "      <td>0.017952</td>\n",
       "      <td>0.185549</td>\n",
       "      <td>0.264725</td>\n",
       "      <td>0.396266</td>\n",
       "      <td>0.890516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>400.0</td>\n",
       "      <td>0.716250</td>\n",
       "      <td>0.405304</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count      mean       std       min       25%       50%       75%       max\n",
       "0  400.0  0.545000  0.271761  0.000000  0.333333  0.500000  0.666667  1.000000\n",
       "1  400.0  0.487500  0.500470  0.000000  0.000000  0.000000  1.000000  1.000000\n",
       "2  400.0  0.290000  0.454330  0.000000  0.000000  0.000000  1.000000  1.000000\n",
       "3  400.0  0.272500  0.445803  0.000000  0.000000  0.000000  1.000000  1.000000\n",
       "4  400.0  0.320000  0.467060  0.000000  0.000000  0.000000  1.000000  1.000000\n",
       "5  400.0  0.757500  0.429132  0.000000  1.000000  1.000000  1.000000  1.000000\n",
       "6  400.0  0.192500  0.394757  0.000000  0.000000  0.000000  0.000000  1.000000\n",
       "7  400.0  0.297500  0.457731  0.000000  0.000000  0.000000  1.000000  1.000000\n",
       "8  400.0  0.304486  0.173502  0.017952  0.185549  0.264725  0.396266  0.890516\n",
       "9  400.0  0.716250  0.405304  0.000000  0.500000  1.000000  1.000000  1.000000"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=scaled_testing).describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngeL/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20011658438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(scaled_testing[:,8], 25, normed=1, facecolor='green', alpha=0.5)\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(np.log(scaled_testing[:,8]), 25, normed=1, facecolor='green', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: total_earnings values were scaled by multiplying by 0.0000036968 and adding -0.115913\n"
     ]
    }
   ],
   "source": [
    "# Print out the adjustment that the scaler applied to the total_earnings column of data\n",
    "print(\"Note: total_earnings values were scaled by multiplying by {:.10f} and adding {:.6f}\".format(scaler.scale_[8], scaler.min_[8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new pandas DataFrame objects from the scaled data\n",
    "scaled_training_df = pd.DataFrame(scaled_training, columns=training_data_df.columns.values)\n",
    "scaled_testing_df = pd.DataFrame(scaled_testing, columns=test_data_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scaled data dataframes to new CSV files\n",
    "scaled_training_df.to_csv(\"./data/sales_data_training_scaled.csv\", index=False)\n",
    "scaled_testing_df.to_csv(\"./data/sales_data_test_scaled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "training_data_df = pd.read_csv(\"./Data/sales_data_training_scaled.csv\")\n",
    "\n",
    "X = training_data_df.drop('total_earnings', axis=1).values #axix=1 means column and axix=0 means row\n",
    "Y = training_data_df[['total_earnings']].values #.values converts the dataframe into numpy\n",
    "print(type(X),type(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model\n",
    "random.seed(10)\n",
    "model = Sequential()\n",
    "model.add(Dense(50,input_dim=9,activation='relu'))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Dense(1,activation='linear')) #we are not applying a non-linear transformation because final output is a single number represents total earning\n",
    "model.compile(loss=\"mean_squared_error\",optimizer=\"adam\",metrics=['mae']) #mse is more sensitive (penalizes) to outliers than mean average error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 1s - loss: 0.0245 - mean_absolute_error: 0.1155\n",
      "Epoch 2/50\n",
      " - 0s - loss: 0.0045 - mean_absolute_error: 0.0476\n",
      "Epoch 3/50\n",
      " - 0s - loss: 0.0016 - mean_absolute_error: 0.0279\n",
      "Epoch 4/50\n",
      " - 0s - loss: 9.7563e-04 - mean_absolute_error: 0.0227\n",
      "Epoch 5/50\n",
      " - 0s - loss: 4.2738e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 6/50\n",
      " - 0s - loss: 2.8621e-04 - mean_absolute_error: 0.0122\n",
      "Epoch 7/50\n",
      " - 0s - loss: 1.8495e-04 - mean_absolute_error: 0.0096\n",
      "Epoch 8/50\n",
      " - 0s - loss: 2.4615e-04 - mean_absolute_error: 0.0116\n",
      "Epoch 9/50\n",
      " - 0s - loss: 1.3091e-04 - mean_absolute_error: 0.0082\n",
      "Epoch 10/50\n",
      " - 0s - loss: 9.0202e-05 - mean_absolute_error: 0.0067\n",
      "Epoch 11/50\n",
      " - 0s - loss: 7.4064e-05 - mean_absolute_error: 0.0062\n",
      "Epoch 12/50\n",
      " - 0s - loss: 6.3998e-05 - mean_absolute_error: 0.0057\n",
      "Epoch 13/50\n",
      " - 0s - loss: 5.3582e-05 - mean_absolute_error: 0.0053\n",
      "Epoch 14/50\n",
      " - 0s - loss: 6.1949e-05 - mean_absolute_error: 0.0058\n",
      "Epoch 15/50\n",
      " - 0s - loss: 6.2381e-05 - mean_absolute_error: 0.0059\n",
      "Epoch 16/50\n",
      " - 0s - loss: 4.2276e-05 - mean_absolute_error: 0.0049\n",
      "Epoch 17/50\n",
      " - 0s - loss: 3.6465e-05 - mean_absolute_error: 0.0045\n",
      "Epoch 18/50\n",
      " - 0s - loss: 3.3163e-05 - mean_absolute_error: 0.0042\n",
      "Epoch 19/50\n",
      " - 0s - loss: 3.1744e-05 - mean_absolute_error: 0.0042\n",
      "Epoch 20/50\n",
      " - 0s - loss: 2.7410e-05 - mean_absolute_error: 0.0039\n",
      "Epoch 21/50\n",
      " - 0s - loss: 2.6741e-05 - mean_absolute_error: 0.0039\n",
      "Epoch 22/50\n",
      " - 0s - loss: 2.9169e-05 - mean_absolute_error: 0.0040\n",
      "Epoch 23/50\n",
      " - 0s - loss: 3.6492e-05 - mean_absolute_error: 0.0046\n",
      "Epoch 24/50\n",
      " - 0s - loss: 3.6339e-05 - mean_absolute_error: 0.0045\n",
      "Epoch 25/50\n",
      " - 0s - loss: 2.8366e-05 - mean_absolute_error: 0.0041\n",
      "Epoch 26/50\n",
      " - 0s - loss: 3.5063e-05 - mean_absolute_error: 0.0045\n",
      "Epoch 27/50\n",
      " - 0s - loss: 3.3121e-05 - mean_absolute_error: 0.0044\n",
      "Epoch 28/50\n",
      " - 0s - loss: 2.7806e-05 - mean_absolute_error: 0.0041\n",
      "Epoch 29/50\n",
      " - 0s - loss: 2.7322e-05 - mean_absolute_error: 0.0041\n",
      "Epoch 30/50\n",
      " - 0s - loss: 2.3476e-05 - mean_absolute_error: 0.0037\n",
      "Epoch 31/50\n",
      " - 0s - loss: 3.4775e-05 - mean_absolute_error: 0.0046\n",
      "Epoch 32/50\n",
      " - 0s - loss: 2.8661e-05 - mean_absolute_error: 0.0041\n",
      "Epoch 33/50\n",
      " - 0s - loss: 2.1804e-05 - mean_absolute_error: 0.0036\n",
      "Epoch 34/50\n",
      " - 0s - loss: 3.8440e-05 - mean_absolute_error: 0.0048\n",
      "Epoch 35/50\n",
      " - 0s - loss: 3.8246e-05 - mean_absolute_error: 0.0046\n",
      "Epoch 36/50\n",
      " - 0s - loss: 4.6384e-05 - mean_absolute_error: 0.0051\n",
      "Epoch 37/50\n",
      " - 0s - loss: 3.0959e-05 - mean_absolute_error: 0.0043\n",
      "Epoch 38/50\n",
      " - 0s - loss: 2.2707e-05 - mean_absolute_error: 0.0036\n",
      "Epoch 39/50\n",
      " - 0s - loss: 2.2758e-05 - mean_absolute_error: 0.0037\n",
      "Epoch 40/50\n",
      " - 0s - loss: 2.7131e-05 - mean_absolute_error: 0.0040\n",
      "Epoch 41/50\n",
      " - 0s - loss: 2.1892e-05 - mean_absolute_error: 0.0036\n",
      "Epoch 42/50\n",
      " - 0s - loss: 8.2040e-05 - mean_absolute_error: 0.0071\n",
      "Epoch 43/50\n",
      " - 0s - loss: 7.2447e-05 - mean_absolute_error: 0.0065\n",
      "Epoch 44/50\n",
      " - 0s - loss: 6.0725e-05 - mean_absolute_error: 0.0059\n",
      "Epoch 45/50\n",
      " - 0s - loss: 3.4614e-05 - mean_absolute_error: 0.0045\n",
      "Epoch 46/50\n",
      " - 0s - loss: 3.5730e-05 - mean_absolute_error: 0.0046\n",
      "Epoch 47/50\n",
      " - 0s - loss: 3.5394e-05 - mean_absolute_error: 0.0045\n",
      "Epoch 48/50\n",
      " - 0s - loss: 2.9718e-05 - mean_absolute_error: 0.0042\n",
      "Epoch 49/50\n",
      " - 0s - loss: 2.5797e-05 - mean_absolute_error: 0.0039\n",
      "Epoch 50/50\n",
      " - 0s - loss: 3.0304e-05 - mean_absolute_error: 0.0042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x200115cb780>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "model.fit(\n",
    "    X,\n",
    "    Y,\n",
    "    epochs=50,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the separate test data set\n",
    "test_data_df = pd.read_csv(\"./data/sales_data_test_scaled.csv\")\n",
    "\n",
    "X_test = test_data_df.drop('total_earnings', axis=1).values\n",
    "Y_test = test_data_df[['total_earnings']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test the mode\n",
    "test_error_rate = model.evaluate(X_test,Y_test,verbose=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error (MSE) for the test data set is: [6.6964792640646916e-05, 0.0055370009690523143]\n"
     ]
    }
   ],
   "source": [
    "test_error_rate =print(\"The mean squared error (MSE) for the test data set is: {}\".format(test_error_rate))\n",
    "#Returns the loss value & metrics values for the model in test mode."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
