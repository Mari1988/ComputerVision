{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras to Build and Train Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will use a neural network to predict diabetes using the Pima Diabetes Dataset.  We will start by training a Random Forest to get a performance baseline.  Then we will use the Keras package to quickly build and train a neural network and compare the performance.  We will see how different network structures affect the performance, training time, and level of overfitting (or underfitting).\n",
    "\n",
    "## UCI Pima Diabetes Dataset\n",
    "\n",
    "* UCI ML Repositiory (http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes)\n",
    "\n",
    "\n",
    "### Attributes: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UCI Pima Diabetes Dataset which has 8 numerical predictors and a binary outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preliminaries\n",
    "\n",
    "from __future__ import absolute_import, division, print_function  # Python 2/3 compatibility\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Import Keras objects for Deep Learning\n",
    "\n",
    "from keras.models  import Sequential, K\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the data set\n",
    "#url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = [\"times_pregnant\", \"glucose_tolerance_test\", \"blood_pressure\", \"skin_thickness\", \"insulin\", \n",
    "         \"bmi\", \"pedigree_function\", \"age\", \"has_diabetes\"]\n",
    "diabetes_df = pd.read_csv(\"./data/diabetes.csv\", names=names,header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>times_pregnant</th>\n",
       "      <th>glucose_tolerance_test</th>\n",
       "      <th>blood_pressure</th>\n",
       "      <th>skin_thickness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree_function</th>\n",
       "      <th>age</th>\n",
       "      <th>has_diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   times_pregnant  glucose_tolerance_test  blood_pressure  skin_thickness  \\\n",
       "0               6                     148              72              35   \n",
       "1               1                      85              66              29   \n",
       "2               8                     183              64               0   \n",
       "3               1                      89              66              23   \n",
       "4               0                     137              40              35   \n",
       "\n",
       "   insulin   bmi  pedigree_function  age  has_diabetes  \n",
       "0        0  33.6              0.627   50             1  \n",
       "1        0  26.6              0.351   31             0  \n",
       "2        0  23.3              0.672   32             1  \n",
       "3       94  28.1              0.167   21             0  \n",
       "4      168  43.1              2.288   33             1  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a peek at the data -- if there are lots of \"NaN\" you may have internet connectivity issues\n",
    "print(diabetes_df.shape)\n",
    "#diabetes_df.sample(5)\n",
    "diabetes_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = diabetes_df.iloc[:, :-1].values\n",
    "y = np.array(diabetes_df[\"has_diabetes\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data to Train, and Test (75%, 25%)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=11111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.34895833333333331, 0.65104166666666663)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y), np.mean(1-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see that about 35% of the patients in this dataset have diabetes, while 65% do not.  This means we can get an accuracy of 65% without any model - just declare that no one has diabetes. We will calculate the ROC-AUC score to evaluate performance of our model, and also look at the accuracy as well to see if we improved upon the 65% accuracy.\n",
    "## Get a baseline performance using Random Forest\n",
    "To begin, and get a baseline for classifier performance:\n",
    "1. Train a Random Forest model with 200 trees on the training data.\n",
    "2. Calculate the accuracy and roc_auc_score of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train the RF Model\n",
    "rf_model = RandomForestClassifier(n_estimators=200)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.766\n",
      "roc-auc is 0.820\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set - both \"hard\" predictions, and the scores (percent of trees voting yes)\n",
    "y_pred_class_rf = rf_model.predict(X_test)\n",
    "y_pred_prob_rf = rf_model.predict_proba(X_test)\n",
    "\n",
    "\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_rf)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_rf[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAHiCAYAAADSwATnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nac4REREDk+YcERExMP8CWOUzVTepgQIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28389066668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_roc(y_test, y_pred, model_name):\n",
    "    fpr, tpr, thr = roc_curve(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.plot(fpr, tpr, 'k-')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=.5)  # roc curve for random model\n",
    "    ax.grid(True)\n",
    "    ax.set(title='ROC Curve for {} on PIMA diabetes problem'.format(model_name),\n",
    "           xlim=[-0.01, 1.01], ylim=[-0.01, 1.01])\n",
    "\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_rf[:, 1], 'RF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Single Hidden Layer Neural Network\n",
    "\n",
    "We will use the Sequential model to quickly build a neural network.  Our first network will be a single layer network.  We have 8 variables, so we set the input shape to 8.  Let's start by having a single hidden layer with 12 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First let's normalize the data\n",
    "## This aids the training of neural nets by providing numerical stability\n",
    "## Random Forest does not need this as it finds a split only, as opposed to performing matrix multiplications\n",
    "normalizer = StandardScaler()\n",
    "X_train_norm = normalizer.fit_transform(X_train)\n",
    "X_test_norm = normalizer.transform(X_test)\n",
    "#type(X_train_norm)\n",
    "#np.savetxt(\"X_train.csv\",X_train,delimiter=\",\")\n",
    "#np.savetxt(\"X_train_norm.csv\",X_train_norm,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the Model \n",
    "# Input size is 8-dimensional\n",
    "# 1 hidden layer, 12 hidden nodes, sigmoid activation\n",
    "# Final layer has just one node with a sigmoid activation (standard for binary classification)\n",
    "\n",
    "model_1 = Sequential([\n",
    "    Dense(12, input_shape=(8,), activation=\"sigmoid\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 121\n",
      "Trainable params: 121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#  This is a nice tool to view the model you have created and count the parameters\n",
    "\n",
    "model_1.summary()\n",
    "#Layer-1 has (8+1)*12 parameters\n",
    "#Layer-2 has (12+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's fit our model for 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/200\n",
      "576/576 [==============================] - 0s 328us/step - loss: 0.5241 - acc: 0.7483 - val_loss: 0.5333 - val_acc: 0.7604\n",
      "Epoch 2/200\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5238 - acc: 0.7483 - val_loss: 0.5331 - val_acc: 0.7604\n",
      "Epoch 3/200\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.5235 - acc: 0.7483 - val_loss: 0.5328 - val_acc: 0.7604\n",
      "Epoch 4/200\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.5232 - acc: 0.7483 - val_loss: 0.5325 - val_acc: 0.7604\n",
      "Epoch 5/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.5229 - acc: 0.7483 - val_loss: 0.5322 - val_acc: 0.7604\n",
      "Epoch 6/200\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5227 - acc: 0.7483 - val_loss: 0.5320 - val_acc: 0.7604\n",
      "Epoch 7/200\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.5224 - acc: 0.7483 - val_loss: 0.5317 - val_acc: 0.7604\n",
      "Epoch 8/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5221 - acc: 0.7465 - val_loss: 0.5314 - val_acc: 0.7604\n",
      "Epoch 9/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5218 - acc: 0.7483 - val_loss: 0.5312 - val_acc: 0.7604\n",
      "Epoch 10/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5215 - acc: 0.7465 - val_loss: 0.5309 - val_acc: 0.7552\n",
      "Epoch 11/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5212 - acc: 0.7465 - val_loss: 0.5306 - val_acc: 0.7500\n",
      "Epoch 12/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5209 - acc: 0.7465 - val_loss: 0.5304 - val_acc: 0.7500\n",
      "Epoch 13/200\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.5207 - acc: 0.7465 - val_loss: 0.5301 - val_acc: 0.7500\n",
      "Epoch 14/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5204 - acc: 0.7483 - val_loss: 0.5299 - val_acc: 0.7500\n",
      "Epoch 15/200\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.5201 - acc: 0.7483 - val_loss: 0.5296 - val_acc: 0.7500\n",
      "Epoch 16/200\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.5198 - acc: 0.7483 - val_loss: 0.5293 - val_acc: 0.7500\n",
      "Epoch 17/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5195 - acc: 0.7483 - val_loss: 0.5291 - val_acc: 0.7500\n",
      "Epoch 18/200\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.5193 - acc: 0.7483 - val_loss: 0.5288 - val_acc: 0.7500\n",
      "Epoch 19/200\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.5190 - acc: 0.7500 - val_loss: 0.5286 - val_acc: 0.7500\n",
      "Epoch 20/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5187 - acc: 0.7500 - val_loss: 0.5283 - val_acc: 0.7500\n",
      "Epoch 21/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.5184 - acc: 0.7500 - val_loss: 0.5281 - val_acc: 0.7552\n",
      "Epoch 22/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5182 - acc: 0.7517 - val_loss: 0.5278 - val_acc: 0.7552\n",
      "Epoch 23/200\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.5179 - acc: 0.7517 - val_loss: 0.5276 - val_acc: 0.7552\n",
      "Epoch 24/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5176 - acc: 0.7535 - val_loss: 0.5273 - val_acc: 0.7552\n",
      "Epoch 25/200\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5174 - acc: 0.7535 - val_loss: 0.5271 - val_acc: 0.7552\n",
      "Epoch 26/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5171 - acc: 0.7535 - val_loss: 0.5268 - val_acc: 0.7552\n",
      "Epoch 27/200\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.5168 - acc: 0.7535 - val_loss: 0.5266 - val_acc: 0.7552\n",
      "Epoch 28/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.5165 - acc: 0.7535 - val_loss: 0.5263 - val_acc: 0.7552\n",
      "Epoch 29/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5163 - acc: 0.7535 - val_loss: 0.5261 - val_acc: 0.7552\n",
      "Epoch 30/200\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5160 - acc: 0.7535 - val_loss: 0.5259 - val_acc: 0.7552\n",
      "Epoch 31/200\n",
      "576/576 [==============================] - 0s 97us/step - loss: 0.5158 - acc: 0.7535 - val_loss: 0.5256 - val_acc: 0.7552\n",
      "Epoch 32/200\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.5155 - acc: 0.7535 - val_loss: 0.5254 - val_acc: 0.7552\n",
      "Epoch 33/200\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.5152 - acc: 0.7535 - val_loss: 0.5251 - val_acc: 0.7552\n",
      "Epoch 34/200\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.5150 - acc: 0.7517 - val_loss: 0.5249 - val_acc: 0.7604\n",
      "Epoch 35/200\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.5147 - acc: 0.7517 - val_loss: 0.5247 - val_acc: 0.7604\n",
      "Epoch 36/200\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.5145 - acc: 0.7535 - val_loss: 0.5244 - val_acc: 0.7604\n",
      "Epoch 37/200\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.5142 - acc: 0.7535 - val_loss: 0.5242 - val_acc: 0.7604\n",
      "Epoch 38/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5140 - acc: 0.7535 - val_loss: 0.5240 - val_acc: 0.7604\n",
      "Epoch 39/200\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.5137 - acc: 0.7535 - val_loss: 0.5237 - val_acc: 0.7604\n",
      "Epoch 40/200\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.5135 - acc: 0.7587 - val_loss: 0.5235 - val_acc: 0.7604\n",
      "Epoch 41/200\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5132 - acc: 0.7587 - val_loss: 0.5233 - val_acc: 0.7604\n",
      "Epoch 42/200\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.5129 - acc: 0.7587 - val_loss: 0.5231 - val_acc: 0.7604\n",
      "Epoch 43/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.5127 - acc: 0.7569 - val_loss: 0.5228 - val_acc: 0.7604\n",
      "Epoch 44/200\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.5124 - acc: 0.7587 - val_loss: 0.5226 - val_acc: 0.7604\n",
      "Epoch 45/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5122 - acc: 0.7604 - val_loss: 0.5224 - val_acc: 0.7604\n",
      "Epoch 46/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5120 - acc: 0.7604 - val_loss: 0.5222 - val_acc: 0.7604\n",
      "Epoch 47/200\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.5117 - acc: 0.7604 - val_loss: 0.5219 - val_acc: 0.7604\n",
      "Epoch 48/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.5115 - acc: 0.7622 - val_loss: 0.5217 - val_acc: 0.7604\n",
      "Epoch 49/200\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.5112 - acc: 0.7622 - val_loss: 0.5215 - val_acc: 0.7604\n",
      "Epoch 50/200\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.5110 - acc: 0.7622 - val_loss: 0.5213 - val_acc: 0.7604\n",
      "Epoch 51/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5108 - acc: 0.7622 - val_loss: 0.5211 - val_acc: 0.7604\n",
      "Epoch 52/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5105 - acc: 0.7622 - val_loss: 0.5209 - val_acc: 0.7604\n",
      "Epoch 53/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5103 - acc: 0.7622 - val_loss: 0.5206 - val_acc: 0.7604\n",
      "Epoch 54/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.5100 - acc: 0.7622 - val_loss: 0.5204 - val_acc: 0.7656\n",
      "Epoch 55/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5098 - acc: 0.7622 - val_loss: 0.5202 - val_acc: 0.7708\n",
      "Epoch 56/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5096 - acc: 0.7587 - val_loss: 0.5200 - val_acc: 0.7708\n",
      "Epoch 57/200\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.5093 - acc: 0.7604 - val_loss: 0.5198 - val_acc: 0.7708\n",
      "Epoch 58/200\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.5091 - acc: 0.7604 - val_loss: 0.5196 - val_acc: 0.7760\n",
      "Epoch 59/200\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.5089 - acc: 0.7639 - val_loss: 0.5194 - val_acc: 0.7760\n",
      "Epoch 60/200\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5086 - acc: 0.7639 - val_loss: 0.5192 - val_acc: 0.7760\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 82us/step - loss: 0.5084 - acc: 0.7639 - val_loss: 0.5190 - val_acc: 0.7760\n",
      "Epoch 62/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.5082 - acc: 0.7656 - val_loss: 0.5188 - val_acc: 0.7760\n",
      "Epoch 63/200\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.5079 - acc: 0.7656 - val_loss: 0.5185 - val_acc: 0.7760\n",
      "Epoch 64/200\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.5077 - acc: 0.7656 - val_loss: 0.5183 - val_acc: 0.7708\n",
      "Epoch 65/200\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.5075 - acc: 0.7656 - val_loss: 0.5181 - val_acc: 0.7708\n",
      "Epoch 66/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5073 - acc: 0.7656 - val_loss: 0.5179 - val_acc: 0.7708\n",
      "Epoch 67/200\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.5070 - acc: 0.7656 - val_loss: 0.5177 - val_acc: 0.7708\n",
      "Epoch 68/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5068 - acc: 0.7656 - val_loss: 0.5175 - val_acc: 0.7708\n",
      "Epoch 69/200\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.5066 - acc: 0.7656 - val_loss: 0.5173 - val_acc: 0.7708\n",
      "Epoch 70/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.5064 - acc: 0.7656 - val_loss: 0.5171 - val_acc: 0.7708\n",
      "Epoch 71/200\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.5062 - acc: 0.7656 - val_loss: 0.5170 - val_acc: 0.7708\n",
      "Epoch 72/200\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.5059 - acc: 0.7656 - val_loss: 0.5168 - val_acc: 0.7708\n",
      "Epoch 73/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.5057 - acc: 0.7656 - val_loss: 0.5166 - val_acc: 0.7708\n",
      "Epoch 74/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.5055 - acc: 0.7656 - val_loss: 0.5164 - val_acc: 0.7708\n",
      "Epoch 75/200\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.5053 - acc: 0.7639 - val_loss: 0.5162 - val_acc: 0.7708\n",
      "Epoch 76/200\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.5051 - acc: 0.7639 - val_loss: 0.5160 - val_acc: 0.7708\n",
      "Epoch 77/200\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.5048 - acc: 0.7639 - val_loss: 0.5158 - val_acc: 0.7708\n",
      "Epoch 78/200\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4749 - acc: 0.812 - 0s 89us/step - loss: 0.5046 - acc: 0.7639 - val_loss: 0.5156 - val_acc: 0.7708\n",
      "Epoch 79/200\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5044 - acc: 0.7639 - val_loss: 0.5154 - val_acc: 0.7708\n",
      "Epoch 80/200\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.5042 - acc: 0.7622 - val_loss: 0.5152 - val_acc: 0.7708\n",
      "Epoch 81/200\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.5040 - acc: 0.7639 - val_loss: 0.5151 - val_acc: 0.7708\n",
      "Epoch 82/200\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.5038 - acc: 0.7622 - val_loss: 0.5149 - val_acc: 0.7708\n",
      "Epoch 83/200\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.5036 - acc: 0.7639 - val_loss: 0.5147 - val_acc: 0.7708\n",
      "Epoch 84/200\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.5034 - acc: 0.7639 - val_loss: 0.5145 - val_acc: 0.7708\n",
      "Epoch 85/200\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.5032 - acc: 0.7639 - val_loss: 0.5143 - val_acc: 0.7708\n",
      "Epoch 86/200\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.5030 - acc: 0.7656 - val_loss: 0.5141 - val_acc: 0.7760\n",
      "Epoch 87/200\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.5028 - acc: 0.7656 - val_loss: 0.5140 - val_acc: 0.7760\n",
      "Epoch 88/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5026 - acc: 0.7674 - val_loss: 0.5138 - val_acc: 0.7760\n",
      "Epoch 89/200\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.5024 - acc: 0.7656 - val_loss: 0.5136 - val_acc: 0.7760\n",
      "Epoch 90/200\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.5022 - acc: 0.7656 - val_loss: 0.5134 - val_acc: 0.7760\n",
      "Epoch 91/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.5019 - acc: 0.7656 - val_loss: 0.5133 - val_acc: 0.7760\n",
      "Epoch 92/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5018 - acc: 0.7656 - val_loss: 0.5131 - val_acc: 0.7760\n",
      "Epoch 93/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5016 - acc: 0.7656 - val_loss: 0.5129 - val_acc: 0.7708\n",
      "Epoch 94/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5014 - acc: 0.7674 - val_loss: 0.5127 - val_acc: 0.7708\n",
      "Epoch 95/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5012 - acc: 0.7674 - val_loss: 0.5126 - val_acc: 0.7708\n",
      "Epoch 96/200\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5010 - acc: 0.7674 - val_loss: 0.5124 - val_acc: 0.7708\n",
      "Epoch 97/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5008 - acc: 0.7674 - val_loss: 0.5122 - val_acc: 0.7708\n",
      "Epoch 98/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5006 - acc: 0.7639 - val_loss: 0.5120 - val_acc: 0.7760\n",
      "Epoch 99/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5004 - acc: 0.7639 - val_loss: 0.5119 - val_acc: 0.7760\n",
      "Epoch 100/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5002 - acc: 0.7639 - val_loss: 0.5117 - val_acc: 0.7760\n",
      "Epoch 101/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5000 - acc: 0.7639 - val_loss: 0.5115 - val_acc: 0.7760\n",
      "Epoch 102/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4998 - acc: 0.7639 - val_loss: 0.5114 - val_acc: 0.7760\n",
      "Epoch 103/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4996 - acc: 0.7639 - val_loss: 0.5112 - val_acc: 0.7760\n",
      "Epoch 104/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4994 - acc: 0.7639 - val_loss: 0.5111 - val_acc: 0.7708\n",
      "Epoch 105/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4992 - acc: 0.7639 - val_loss: 0.5109 - val_acc: 0.7708\n",
      "Epoch 106/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4991 - acc: 0.7639 - val_loss: 0.5107 - val_acc: 0.7708\n",
      "Epoch 107/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4989 - acc: 0.7639 - val_loss: 0.5106 - val_acc: 0.7708\n",
      "Epoch 108/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4987 - acc: 0.7639 - val_loss: 0.5104 - val_acc: 0.7708\n",
      "Epoch 109/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4985 - acc: 0.7639 - val_loss: 0.5102 - val_acc: 0.7708\n",
      "Epoch 110/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4983 - acc: 0.7639 - val_loss: 0.5101 - val_acc: 0.7708\n",
      "Epoch 111/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4981 - acc: 0.7639 - val_loss: 0.5099 - val_acc: 0.7708\n",
      "Epoch 112/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4979 - acc: 0.7639 - val_loss: 0.5098 - val_acc: 0.7708\n",
      "Epoch 113/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4978 - acc: 0.7639 - val_loss: 0.5096 - val_acc: 0.7708\n",
      "Epoch 114/200\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4976 - acc: 0.7639 - val_loss: 0.5095 - val_acc: 0.7708\n",
      "Epoch 115/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4974 - acc: 0.7639 - val_loss: 0.5093 - val_acc: 0.7708\n",
      "Epoch 116/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4972 - acc: 0.7639 - val_loss: 0.5092 - val_acc: 0.7708\n",
      "Epoch 117/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4970 - acc: 0.7639 - val_loss: 0.5090 - val_acc: 0.7708\n",
      "Epoch 118/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4969 - acc: 0.7674 - val_loss: 0.5089 - val_acc: 0.7708\n",
      "Epoch 119/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4967 - acc: 0.7674 - val_loss: 0.5087 - val_acc: 0.7708\n",
      "Epoch 120/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4965 - acc: 0.7674 - val_loss: 0.5086 - val_acc: 0.7708\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 63us/step - loss: 0.4964 - acc: 0.7674 - val_loss: 0.5084 - val_acc: 0.7760\n",
      "Epoch 122/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4962 - acc: 0.7674 - val_loss: 0.5083 - val_acc: 0.7812\n",
      "Epoch 123/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4960 - acc: 0.7674 - val_loss: 0.5081 - val_acc: 0.7812\n",
      "Epoch 124/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4958 - acc: 0.7674 - val_loss: 0.5080 - val_acc: 0.7812\n",
      "Epoch 125/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4956 - acc: 0.7674 - val_loss: 0.5078 - val_acc: 0.7812\n",
      "Epoch 126/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4955 - acc: 0.7674 - val_loss: 0.5077 - val_acc: 0.7812\n",
      "Epoch 127/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4953 - acc: 0.7674 - val_loss: 0.5075 - val_acc: 0.7812\n",
      "Epoch 128/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4951 - acc: 0.7674 - val_loss: 0.5074 - val_acc: 0.7812\n",
      "Epoch 129/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4950 - acc: 0.7674 - val_loss: 0.5072 - val_acc: 0.7812\n",
      "Epoch 130/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4948 - acc: 0.7674 - val_loss: 0.5071 - val_acc: 0.7812\n",
      "Epoch 131/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4946 - acc: 0.7674 - val_loss: 0.5070 - val_acc: 0.7812\n",
      "Epoch 132/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4945 - acc: 0.7674 - val_loss: 0.5068 - val_acc: 0.7812\n",
      "Epoch 133/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4943 - acc: 0.7674 - val_loss: 0.5067 - val_acc: 0.7865\n",
      "Epoch 134/200\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.5523 - acc: 0.718 - 0s 56us/step - loss: 0.4941 - acc: 0.7691 - val_loss: 0.5065 - val_acc: 0.7865\n",
      "Epoch 135/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4940 - acc: 0.7691 - val_loss: 0.5064 - val_acc: 0.7865\n",
      "Epoch 136/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4938 - acc: 0.7708 - val_loss: 0.5063 - val_acc: 0.7865\n",
      "Epoch 137/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4937 - acc: 0.7691 - val_loss: 0.5061 - val_acc: 0.7865\n",
      "Epoch 138/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4935 - acc: 0.7691 - val_loss: 0.5060 - val_acc: 0.7865\n",
      "Epoch 139/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4933 - acc: 0.7674 - val_loss: 0.5059 - val_acc: 0.7865\n",
      "Epoch 140/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4932 - acc: 0.7691 - val_loss: 0.5057 - val_acc: 0.7865\n",
      "Epoch 141/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4930 - acc: 0.7691 - val_loss: 0.5056 - val_acc: 0.7812\n",
      "Epoch 142/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4929 - acc: 0.7691 - val_loss: 0.5055 - val_acc: 0.7812\n",
      "Epoch 143/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4927 - acc: 0.7691 - val_loss: 0.5053 - val_acc: 0.7812\n",
      "Epoch 144/200\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4925 - acc: 0.7691 - val_loss: 0.5052 - val_acc: 0.7812\n",
      "Epoch 145/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4924 - acc: 0.7691 - val_loss: 0.5051 - val_acc: 0.7812\n",
      "Epoch 146/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4922 - acc: 0.7691 - val_loss: 0.5049 - val_acc: 0.7865\n",
      "Epoch 147/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4921 - acc: 0.7691 - val_loss: 0.5048 - val_acc: 0.7865\n",
      "Epoch 148/200\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4526 - acc: 0.843 - 0s 71us/step - loss: 0.4919 - acc: 0.7691 - val_loss: 0.5047 - val_acc: 0.7865\n",
      "Epoch 149/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4918 - acc: 0.7691 - val_loss: 0.5046 - val_acc: 0.7865\n",
      "Epoch 150/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4916 - acc: 0.7691 - val_loss: 0.5044 - val_acc: 0.7865\n",
      "Epoch 151/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4915 - acc: 0.7691 - val_loss: 0.5043 - val_acc: 0.7865\n",
      "Epoch 152/200\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4913 - acc: 0.7691 - val_loss: 0.5042 - val_acc: 0.7865\n",
      "Epoch 153/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4912 - acc: 0.7708 - val_loss: 0.5041 - val_acc: 0.7865\n",
      "Epoch 154/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4910 - acc: 0.7708 - val_loss: 0.5039 - val_acc: 0.7917\n",
      "Epoch 155/200\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4909 - acc: 0.7708 - val_loss: 0.5038 - val_acc: 0.7917\n",
      "Epoch 156/200\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4907 - acc: 0.7691 - val_loss: 0.5037 - val_acc: 0.7917\n",
      "Epoch 157/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4905 - acc: 0.7726 - val_loss: 0.5036 - val_acc: 0.7917\n",
      "Epoch 158/200\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4904 - acc: 0.7743 - val_loss: 0.5035 - val_acc: 0.7917\n",
      "Epoch 159/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4903 - acc: 0.7743 - val_loss: 0.5033 - val_acc: 0.7917\n",
      "Epoch 160/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4901 - acc: 0.7743 - val_loss: 0.5032 - val_acc: 0.7917\n",
      "Epoch 161/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4900 - acc: 0.7708 - val_loss: 0.5031 - val_acc: 0.7917\n",
      "Epoch 162/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4898 - acc: 0.7708 - val_loss: 0.5030 - val_acc: 0.7917\n",
      "Epoch 163/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4897 - acc: 0.7708 - val_loss: 0.5029 - val_acc: 0.7917\n",
      "Epoch 164/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4896 - acc: 0.7708 - val_loss: 0.5027 - val_acc: 0.7917\n",
      "Epoch 165/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4894 - acc: 0.7708 - val_loss: 0.5026 - val_acc: 0.7917\n",
      "Epoch 166/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4893 - acc: 0.7708 - val_loss: 0.5025 - val_acc: 0.7917\n",
      "Epoch 167/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4891 - acc: 0.7708 - val_loss: 0.5024 - val_acc: 0.7917\n",
      "Epoch 168/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4890 - acc: 0.7708 - val_loss: 0.5023 - val_acc: 0.7917\n",
      "Epoch 169/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4888 - acc: 0.7708 - val_loss: 0.5022 - val_acc: 0.7917\n",
      "Epoch 170/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4887 - acc: 0.7708 - val_loss: 0.5021 - val_acc: 0.7917\n",
      "Epoch 171/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4886 - acc: 0.7708 - val_loss: 0.5020 - val_acc: 0.7917\n",
      "Epoch 172/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4884 - acc: 0.7708 - val_loss: 0.5018 - val_acc: 0.7917\n",
      "Epoch 173/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4883 - acc: 0.7708 - val_loss: 0.5017 - val_acc: 0.7917\n",
      "Epoch 174/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4882 - acc: 0.7708 - val_loss: 0.5016 - val_acc: 0.7917\n",
      "Epoch 175/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4880 - acc: 0.7708 - val_loss: 0.5015 - val_acc: 0.7917\n",
      "Epoch 176/200\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.5489 - acc: 0.781 - 0s 60us/step - loss: 0.4879 - acc: 0.7708 - val_loss: 0.5014 - val_acc: 0.7917\n",
      "Epoch 177/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4877 - acc: 0.7708 - val_loss: 0.5013 - val_acc: 0.7917\n",
      "Epoch 178/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4876 - acc: 0.7708 - val_loss: 0.5012 - val_acc: 0.7917\n",
      "Epoch 179/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4875 - acc: 0.7708 - val_loss: 0.5011 - val_acc: 0.7917\n",
      "Epoch 180/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 71us/step - loss: 0.4874 - acc: 0.7708 - val_loss: 0.5010 - val_acc: 0.7917\n",
      "Epoch 181/200\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4872 - acc: 0.7726 - val_loss: 0.5009 - val_acc: 0.7917\n",
      "Epoch 182/200\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4871 - acc: 0.7726 - val_loss: 0.5008 - val_acc: 0.7917\n",
      "Epoch 183/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4870 - acc: 0.7743 - val_loss: 0.5007 - val_acc: 0.7917\n",
      "Epoch 184/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4868 - acc: 0.7743 - val_loss: 0.5006 - val_acc: 0.7917\n",
      "Epoch 185/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4867 - acc: 0.7743 - val_loss: 0.5005 - val_acc: 0.7917\n",
      "Epoch 186/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4866 - acc: 0.7743 - val_loss: 0.5004 - val_acc: 0.7917\n",
      "Epoch 187/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4864 - acc: 0.7743 - val_loss: 0.5003 - val_acc: 0.7917\n",
      "Epoch 188/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4863 - acc: 0.7743 - val_loss: 0.5002 - val_acc: 0.7917\n",
      "Epoch 189/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4862 - acc: 0.7743 - val_loss: 0.5001 - val_acc: 0.7917\n",
      "Epoch 190/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4861 - acc: 0.7760 - val_loss: 0.5000 - val_acc: 0.7917\n",
      "Epoch 191/200\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4859 - acc: 0.7743 - val_loss: 0.4999 - val_acc: 0.7917\n",
      "Epoch 192/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4858 - acc: 0.7743 - val_loss: 0.4998 - val_acc: 0.7865\n",
      "Epoch 193/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4857 - acc: 0.7760 - val_loss: 0.4997 - val_acc: 0.7865\n",
      "Epoch 194/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4856 - acc: 0.7760 - val_loss: 0.4996 - val_acc: 0.7865\n",
      "Epoch 195/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4854 - acc: 0.7760 - val_loss: 0.4995 - val_acc: 0.7865\n",
      "Epoch 196/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4853 - acc: 0.7760 - val_loss: 0.4994 - val_acc: 0.7865\n",
      "Epoch 197/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4852 - acc: 0.7760 - val_loss: 0.4993 - val_acc: 0.7865\n",
      "Epoch 198/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4851 - acc: 0.7760 - val_loss: 0.4992 - val_acc: 0.7917\n",
      "Epoch 199/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4850 - acc: 0.7760 - val_loss: 0.4991 - val_acc: 0.7917\n",
      "Epoch 200/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4848 - acc: 0.7760 - val_loss: 0.4990 - val_acc: 0.7917\n"
     ]
    }
   ],
   "source": [
    "# Fit(Train) the Model\n",
    "\n",
    "# Compile the model with Optimizer, Loss Function and Metrics\n",
    "# Roc-Auc is not available in Keras as an off the shelf metric yet, so we will skip it here.\n",
    "\n",
    "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_1 = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=200)\n",
    "# the fit function returns the run history. \n",
    "# It is very convenient, as it contains information about the model fit, iterations etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Like we did for the Random Forest, we generate two kinds of predictions\n",
    "#  One is a hard decision, the other is a probabilitistic score.\n",
    "\n",
    "y_pred_class_nn_1 = model_1.predict_classes(X_test_norm)\n",
    "y_pred_prob_nn_1 = model_1.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out the outputs to get a feel for how keras apis work.\n",
    "y_pred_class_nn_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4293254 ],\n",
       "       [ 0.5714699 ],\n",
       "       [ 0.27412999],\n",
       "       [ 0.29393944],\n",
       "       [ 0.22942311],\n",
       "       [ 0.43258914],\n",
       "       [ 0.11373448],\n",
       "       [ 0.30001077],\n",
       "       [ 0.73751646],\n",
       "       [ 0.25819999]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob_nn_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.792\n",
      "roc-auc is 0.825\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAHiCAYAAADSwATnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxvw/hLnsUsU6DJUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28388e34438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print model performance and plot the roc curve\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_nn_1, 'NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be some variation in exact numbers due to randomness, but you should get results in the same ballpark as the Random Forest - between 75% and 85% accuracy, between .8 and .9 for AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `run_hist_1` object that was created, specifically its `history` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_hist_1.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training loss and the validation loss over the different epochs and see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x28389f76940>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRGJEQV9EJEYU9EVEYkRBX0QkRv4/1Vk2++J5UGoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28388e18f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the losses are still going down on both the training set and the validation set.  This suggests that the model might benefit from further training.  Let's train the model a little more and see what happens. Note that it will pick up from where it left off. Train for 1000 more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4847 - acc: 0.7778 - val_loss: 0.4989 - val_acc: 0.7917\n",
      "Epoch 2/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4846 - acc: 0.7778 - val_loss: 0.4988 - val_acc: 0.7865\n",
      "Epoch 3/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4845 - acc: 0.7760 - val_loss: 0.4987 - val_acc: 0.7865\n",
      "Epoch 4/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4844 - acc: 0.7795 - val_loss: 0.4986 - val_acc: 0.7865\n",
      "Epoch 5/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4842 - acc: 0.7795 - val_loss: 0.4985 - val_acc: 0.7865\n",
      "Epoch 6/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4841 - acc: 0.7795 - val_loss: 0.4984 - val_acc: 0.7812\n",
      "Epoch 7/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4840 - acc: 0.7795 - val_loss: 0.4984 - val_acc: 0.7812\n",
      "Epoch 8/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4839 - acc: 0.7795 - val_loss: 0.4983 - val_acc: 0.7812\n",
      "Epoch 9/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4838 - acc: 0.7795 - val_loss: 0.4982 - val_acc: 0.7812\n",
      "Epoch 10/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4836 - acc: 0.7795 - val_loss: 0.4981 - val_acc: 0.7812\n",
      "Epoch 11/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4835 - acc: 0.7795 - val_loss: 0.4980 - val_acc: 0.7812\n",
      "Epoch 12/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4834 - acc: 0.7812 - val_loss: 0.4979 - val_acc: 0.7812\n",
      "Epoch 13/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4833 - acc: 0.7795 - val_loss: 0.4978 - val_acc: 0.7812\n",
      "Epoch 14/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4832 - acc: 0.7795 - val_loss: 0.4977 - val_acc: 0.7812\n",
      "Epoch 15/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4831 - acc: 0.7795 - val_loss: 0.4977 - val_acc: 0.7812\n",
      "Epoch 16/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4830 - acc: 0.7795 - val_loss: 0.4976 - val_acc: 0.7812\n",
      "Epoch 17/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4829 - acc: 0.7812 - val_loss: 0.4975 - val_acc: 0.7812\n",
      "Epoch 18/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4827 - acc: 0.7812 - val_loss: 0.4974 - val_acc: 0.7812\n",
      "Epoch 19/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4826 - acc: 0.7795 - val_loss: 0.4973 - val_acc: 0.7812\n",
      "Epoch 20/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4825 - acc: 0.7812 - val_loss: 0.4972 - val_acc: 0.7812\n",
      "Epoch 21/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4824 - acc: 0.7812 - val_loss: 0.4972 - val_acc: 0.7812\n",
      "Epoch 22/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4823 - acc: 0.7812 - val_loss: 0.4971 - val_acc: 0.7812\n",
      "Epoch 23/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4822 - acc: 0.7812 - val_loss: 0.4970 - val_acc: 0.7812\n",
      "Epoch 24/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4821 - acc: 0.7812 - val_loss: 0.4969 - val_acc: 0.7812\n",
      "Epoch 25/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4820 - acc: 0.7812 - val_loss: 0.4968 - val_acc: 0.7812\n",
      "Epoch 26/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4819 - acc: 0.7812 - val_loss: 0.4967 - val_acc: 0.7812\n",
      "Epoch 27/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4818 - acc: 0.7812 - val_loss: 0.4967 - val_acc: 0.7812\n",
      "Epoch 28/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4817 - acc: 0.7812 - val_loss: 0.4966 - val_acc: 0.7812\n",
      "Epoch 29/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4816 - acc: 0.7847 - val_loss: 0.4965 - val_acc: 0.7812\n",
      "Epoch 30/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4815 - acc: 0.7812 - val_loss: 0.4964 - val_acc: 0.7812\n",
      "Epoch 31/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4814 - acc: 0.7847 - val_loss: 0.4964 - val_acc: 0.7812\n",
      "Epoch 32/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4813 - acc: 0.7847 - val_loss: 0.4963 - val_acc: 0.7812\n",
      "Epoch 33/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4812 - acc: 0.7847 - val_loss: 0.4962 - val_acc: 0.7812\n",
      "Epoch 34/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4810 - acc: 0.7847 - val_loss: 0.4961 - val_acc: 0.7812\n",
      "Epoch 35/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4809 - acc: 0.7847 - val_loss: 0.4960 - val_acc: 0.7812\n",
      "Epoch 36/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4808 - acc: 0.7847 - val_loss: 0.4960 - val_acc: 0.7812\n",
      "Epoch 37/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4807 - acc: 0.7865 - val_loss: 0.4959 - val_acc: 0.7812\n",
      "Epoch 38/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4807 - acc: 0.7847 - val_loss: 0.4958 - val_acc: 0.7812\n",
      "Epoch 39/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4805 - acc: 0.7847 - val_loss: 0.4958 - val_acc: 0.7812\n",
      "Epoch 40/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4805 - acc: 0.7830 - val_loss: 0.4957 - val_acc: 0.7812\n",
      "Epoch 41/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4803 - acc: 0.7830 - val_loss: 0.4956 - val_acc: 0.7812\n",
      "Epoch 42/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4802 - acc: 0.7830 - val_loss: 0.4955 - val_acc: 0.7812\n",
      "Epoch 43/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4801 - acc: 0.7830 - val_loss: 0.4955 - val_acc: 0.7812\n",
      "Epoch 44/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4800 - acc: 0.7830 - val_loss: 0.4954 - val_acc: 0.7812\n",
      "Epoch 45/1000\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4799 - acc: 0.7830 - val_loss: 0.4953 - val_acc: 0.7812\n",
      "Epoch 46/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4798 - acc: 0.7830 - val_loss: 0.4952 - val_acc: 0.7812\n",
      "Epoch 47/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4798 - acc: 0.7830 - val_loss: 0.4952 - val_acc: 0.7812\n",
      "Epoch 48/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4797 - acc: 0.7830 - val_loss: 0.4951 - val_acc: 0.7812\n",
      "Epoch 49/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4796 - acc: 0.7830 - val_loss: 0.4950 - val_acc: 0.7812\n",
      "Epoch 50/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4795 - acc: 0.7830 - val_loss: 0.4950 - val_acc: 0.7812\n",
      "Epoch 51/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4794 - acc: 0.7830 - val_loss: 0.4949 - val_acc: 0.7812\n",
      "Epoch 52/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4793 - acc: 0.7830 - val_loss: 0.4948 - val_acc: 0.7812\n",
      "Epoch 53/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4792 - acc: 0.7830 - val_loss: 0.4948 - val_acc: 0.7812\n",
      "Epoch 54/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4791 - acc: 0.7830 - val_loss: 0.4947 - val_acc: 0.7812\n",
      "Epoch 55/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4790 - acc: 0.7830 - val_loss: 0.4946 - val_acc: 0.7812\n",
      "Epoch 56/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4789 - acc: 0.7830 - val_loss: 0.4946 - val_acc: 0.7812\n",
      "Epoch 57/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4788 - acc: 0.7830 - val_loss: 0.4945 - val_acc: 0.7812\n",
      "Epoch 58/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4787 - acc: 0.7830 - val_loss: 0.4944 - val_acc: 0.7812\n",
      "Epoch 59/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4786 - acc: 0.7830 - val_loss: 0.4944 - val_acc: 0.7812\n",
      "Epoch 60/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4785 - acc: 0.7830 - val_loss: 0.4943 - val_acc: 0.7812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4784 - acc: 0.7830 - val_loss: 0.4942 - val_acc: 0.7812\n",
      "Epoch 62/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4784 - acc: 0.7830 - val_loss: 0.4942 - val_acc: 0.7812\n",
      "Epoch 63/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4783 - acc: 0.7830 - val_loss: 0.4941 - val_acc: 0.7812\n",
      "Epoch 64/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4782 - acc: 0.7830 - val_loss: 0.4941 - val_acc: 0.7812\n",
      "Epoch 65/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4781 - acc: 0.7830 - val_loss: 0.4940 - val_acc: 0.7865\n",
      "Epoch 66/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4780 - acc: 0.7830 - val_loss: 0.4939 - val_acc: 0.7812\n",
      "Epoch 67/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4779 - acc: 0.7830 - val_loss: 0.4939 - val_acc: 0.7812\n",
      "Epoch 68/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4778 - acc: 0.7830 - val_loss: 0.4938 - val_acc: 0.7760\n",
      "Epoch 69/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4777 - acc: 0.7830 - val_loss: 0.4937 - val_acc: 0.7760\n",
      "Epoch 70/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4777 - acc: 0.7830 - val_loss: 0.4937 - val_acc: 0.7760\n",
      "Epoch 71/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4776 - acc: 0.7830 - val_loss: 0.4936 - val_acc: 0.7760\n",
      "Epoch 72/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4775 - acc: 0.7830 - val_loss: 0.4936 - val_acc: 0.7760\n",
      "Epoch 73/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4774 - acc: 0.7830 - val_loss: 0.4935 - val_acc: 0.7760\n",
      "Epoch 74/1000\n",
      "576/576 [==============================] - 0s 98us/step - loss: 0.4773 - acc: 0.7830 - val_loss: 0.4934 - val_acc: 0.7760\n",
      "Epoch 75/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4772 - acc: 0.7830 - val_loss: 0.4934 - val_acc: 0.7760\n",
      "Epoch 76/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4771 - acc: 0.7830 - val_loss: 0.4933 - val_acc: 0.7760\n",
      "Epoch 77/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4770 - acc: 0.7830 - val_loss: 0.4933 - val_acc: 0.7760\n",
      "Epoch 78/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4770 - acc: 0.7830 - val_loss: 0.4932 - val_acc: 0.7708\n",
      "Epoch 79/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4769 - acc: 0.7830 - val_loss: 0.4932 - val_acc: 0.7708\n",
      "Epoch 80/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4768 - acc: 0.7830 - val_loss: 0.4931 - val_acc: 0.7708\n",
      "Epoch 81/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4767 - acc: 0.7830 - val_loss: 0.4930 - val_acc: 0.7708\n",
      "Epoch 82/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4766 - acc: 0.7830 - val_loss: 0.4930 - val_acc: 0.7708\n",
      "Epoch 83/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4765 - acc: 0.7830 - val_loss: 0.4929 - val_acc: 0.7708\n",
      "Epoch 84/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4765 - acc: 0.7830 - val_loss: 0.4929 - val_acc: 0.7708\n",
      "Epoch 85/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.5035 - acc: 0.781 - 0s 57us/step - loss: 0.4764 - acc: 0.7830 - val_loss: 0.4928 - val_acc: 0.7708\n",
      "Epoch 86/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4763 - acc: 0.7830 - val_loss: 0.4928 - val_acc: 0.7708\n",
      "Epoch 87/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4762 - acc: 0.7830 - val_loss: 0.4927 - val_acc: 0.7708\n",
      "Epoch 88/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4762 - acc: 0.7830 - val_loss: 0.4927 - val_acc: 0.7708\n",
      "Epoch 89/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4761 - acc: 0.7830 - val_loss: 0.4926 - val_acc: 0.7708\n",
      "Epoch 90/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4760 - acc: 0.7830 - val_loss: 0.4926 - val_acc: 0.7708\n",
      "Epoch 91/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4759 - acc: 0.7830 - val_loss: 0.4925 - val_acc: 0.7708\n",
      "Epoch 92/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4758 - acc: 0.7830 - val_loss: 0.4925 - val_acc: 0.7708\n",
      "Epoch 93/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4758 - acc: 0.7830 - val_loss: 0.4924 - val_acc: 0.7708\n",
      "Epoch 94/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4757 - acc: 0.7847 - val_loss: 0.4923 - val_acc: 0.7708\n",
      "Epoch 95/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4756 - acc: 0.7847 - val_loss: 0.4923 - val_acc: 0.7708\n",
      "Epoch 96/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4755 - acc: 0.7847 - val_loss: 0.4922 - val_acc: 0.7708\n",
      "Epoch 97/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4755 - acc: 0.7847 - val_loss: 0.4922 - val_acc: 0.7708\n",
      "Epoch 98/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4754 - acc: 0.7847 - val_loss: 0.4921 - val_acc: 0.7708\n",
      "Epoch 99/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4753 - acc: 0.7847 - val_loss: 0.4921 - val_acc: 0.7708\n",
      "Epoch 100/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.3920 - acc: 0.812 - 0s 73us/step - loss: 0.4752 - acc: 0.7830 - val_loss: 0.4920 - val_acc: 0.7708\n",
      "Epoch 101/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4751 - acc: 0.7830 - val_loss: 0.4920 - val_acc: 0.7708\n",
      "Epoch 102/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4751 - acc: 0.7830 - val_loss: 0.4919 - val_acc: 0.7708\n",
      "Epoch 103/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4750 - acc: 0.7830 - val_loss: 0.4919 - val_acc: 0.7708\n",
      "Epoch 104/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4749 - acc: 0.7812 - val_loss: 0.4918 - val_acc: 0.7708\n",
      "Epoch 105/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4748 - acc: 0.7830 - val_loss: 0.4918 - val_acc: 0.7708\n",
      "Epoch 106/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4748 - acc: 0.7830 - val_loss: 0.4918 - val_acc: 0.7708\n",
      "Epoch 107/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4747 - acc: 0.7830 - val_loss: 0.4917 - val_acc: 0.7708\n",
      "Epoch 108/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4746 - acc: 0.7830 - val_loss: 0.4917 - val_acc: 0.7708\n",
      "Epoch 109/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4745 - acc: 0.7830 - val_loss: 0.4916 - val_acc: 0.7708\n",
      "Epoch 110/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4745 - acc: 0.7812 - val_loss: 0.4916 - val_acc: 0.7708\n",
      "Epoch 111/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4744 - acc: 0.7812 - val_loss: 0.4915 - val_acc: 0.7708\n",
      "Epoch 112/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4743 - acc: 0.7812 - val_loss: 0.4915 - val_acc: 0.7708\n",
      "Epoch 113/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4743 - acc: 0.7812 - val_loss: 0.4914 - val_acc: 0.7708\n",
      "Epoch 114/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4742 - acc: 0.7812 - val_loss: 0.4914 - val_acc: 0.7708\n",
      "Epoch 115/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4741 - acc: 0.7812 - val_loss: 0.4913 - val_acc: 0.7708\n",
      "Epoch 116/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4741 - acc: 0.7812 - val_loss: 0.4913 - val_acc: 0.7708\n",
      "Epoch 117/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4740 - acc: 0.7812 - val_loss: 0.4912 - val_acc: 0.7708\n",
      "Epoch 118/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4739 - acc: 0.7812 - val_loss: 0.4912 - val_acc: 0.7708\n",
      "Epoch 119/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4738 - acc: 0.7812 - val_loss: 0.4912 - val_acc: 0.7708\n",
      "Epoch 120/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 58us/step - loss: 0.4738 - acc: 0.7812 - val_loss: 0.4911 - val_acc: 0.7708\n",
      "Epoch 121/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4737 - acc: 0.7830 - val_loss: 0.4911 - val_acc: 0.7708\n",
      "Epoch 122/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4736 - acc: 0.7830 - val_loss: 0.4910 - val_acc: 0.7708\n",
      "Epoch 123/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4735 - acc: 0.7830 - val_loss: 0.4910 - val_acc: 0.7708\n",
      "Epoch 124/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4735 - acc: 0.7830 - val_loss: 0.4909 - val_acc: 0.7708\n",
      "Epoch 125/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4734 - acc: 0.7830 - val_loss: 0.4909 - val_acc: 0.7708\n",
      "Epoch 126/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4734 - acc: 0.7830 - val_loss: 0.4909 - val_acc: 0.7708\n",
      "Epoch 127/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4733 - acc: 0.7830 - val_loss: 0.4908 - val_acc: 0.7708\n",
      "Epoch 128/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.5413 - acc: 0.718 - 0s 61us/step - loss: 0.4732 - acc: 0.7830 - val_loss: 0.4908 - val_acc: 0.7708\n",
      "Epoch 129/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4732 - acc: 0.7830 - val_loss: 0.4907 - val_acc: 0.7708\n",
      "Epoch 130/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4731 - acc: 0.7812 - val_loss: 0.4907 - val_acc: 0.7708\n",
      "Epoch 131/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4730 - acc: 0.7812 - val_loss: 0.4907 - val_acc: 0.7708\n",
      "Epoch 132/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4730 - acc: 0.7830 - val_loss: 0.4906 - val_acc: 0.7708\n",
      "Epoch 133/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4729 - acc: 0.7812 - val_loss: 0.4906 - val_acc: 0.7708\n",
      "Epoch 134/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4728 - acc: 0.7812 - val_loss: 0.4905 - val_acc: 0.7708\n",
      "Epoch 135/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4728 - acc: 0.7812 - val_loss: 0.4905 - val_acc: 0.7708\n",
      "Epoch 136/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4727 - acc: 0.7812 - val_loss: 0.4905 - val_acc: 0.7708\n",
      "Epoch 137/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4726 - acc: 0.7812 - val_loss: 0.4904 - val_acc: 0.7708\n",
      "Epoch 138/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4726 - acc: 0.7812 - val_loss: 0.4904 - val_acc: 0.7708\n",
      "Epoch 139/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4725 - acc: 0.7812 - val_loss: 0.4903 - val_acc: 0.7708\n",
      "Epoch 140/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4725 - acc: 0.7812 - val_loss: 0.4903 - val_acc: 0.7708\n",
      "Epoch 141/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4724 - acc: 0.7812 - val_loss: 0.4903 - val_acc: 0.7708\n",
      "Epoch 142/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4723 - acc: 0.7795 - val_loss: 0.4902 - val_acc: 0.7708\n",
      "Epoch 143/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4722 - acc: 0.7795 - val_loss: 0.4902 - val_acc: 0.7708\n",
      "Epoch 144/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4722 - acc: 0.7795 - val_loss: 0.4902 - val_acc: 0.7708\n",
      "Epoch 145/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4721 - acc: 0.7795 - val_loss: 0.4901 - val_acc: 0.7708\n",
      "Epoch 146/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4721 - acc: 0.7795 - val_loss: 0.4901 - val_acc: 0.7708\n",
      "Epoch 147/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4720 - acc: 0.7795 - val_loss: 0.4900 - val_acc: 0.7708\n",
      "Epoch 148/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4720 - acc: 0.7795 - val_loss: 0.4900 - val_acc: 0.7708\n",
      "Epoch 149/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4719 - acc: 0.7795 - val_loss: 0.4900 - val_acc: 0.7708\n",
      "Epoch 150/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4718 - acc: 0.7795 - val_loss: 0.4899 - val_acc: 0.7708\n",
      "Epoch 151/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4718 - acc: 0.7795 - val_loss: 0.4899 - val_acc: 0.7708\n",
      "Epoch 152/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4717 - acc: 0.7795 - val_loss: 0.4899 - val_acc: 0.7708\n",
      "Epoch 153/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4717 - acc: 0.7795 - val_loss: 0.4898 - val_acc: 0.7708\n",
      "Epoch 154/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4716 - acc: 0.7795 - val_loss: 0.4898 - val_acc: 0.7708\n",
      "Epoch 155/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4715 - acc: 0.7795 - val_loss: 0.4898 - val_acc: 0.7708\n",
      "Epoch 156/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4715 - acc: 0.7795 - val_loss: 0.4897 - val_acc: 0.7708\n",
      "Epoch 157/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4714 - acc: 0.7795 - val_loss: 0.4897 - val_acc: 0.7708\n",
      "Epoch 158/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4714 - acc: 0.7795 - val_loss: 0.4897 - val_acc: 0.7708\n",
      "Epoch 159/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4713 - acc: 0.7795 - val_loss: 0.4896 - val_acc: 0.7708\n",
      "Epoch 160/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4712 - acc: 0.7795 - val_loss: 0.4896 - val_acc: 0.7708\n",
      "Epoch 161/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4712 - acc: 0.7795 - val_loss: 0.4896 - val_acc: 0.7708\n",
      "Epoch 162/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4711 - acc: 0.7795 - val_loss: 0.4895 - val_acc: 0.7708\n",
      "Epoch 163/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4710 - acc: 0.7795 - val_loss: 0.4895 - val_acc: 0.7708\n",
      "Epoch 164/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4710 - acc: 0.7795 - val_loss: 0.4895 - val_acc: 0.7708\n",
      "Epoch 165/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4710 - acc: 0.7795 - val_loss: 0.4894 - val_acc: 0.7708\n",
      "Epoch 166/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4709 - acc: 0.7795 - val_loss: 0.4894 - val_acc: 0.7708\n",
      "Epoch 167/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4709 - acc: 0.7795 - val_loss: 0.4894 - val_acc: 0.7708\n",
      "Epoch 168/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4708 - acc: 0.7795 - val_loss: 0.4893 - val_acc: 0.7708\n",
      "Epoch 169/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4707 - acc: 0.7795 - val_loss: 0.4893 - val_acc: 0.7708\n",
      "Epoch 170/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4707 - acc: 0.7795 - val_loss: 0.4893 - val_acc: 0.7708\n",
      "Epoch 171/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4706 - acc: 0.7795 - val_loss: 0.4892 - val_acc: 0.7708\n",
      "Epoch 172/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4706 - acc: 0.7795 - val_loss: 0.4892 - val_acc: 0.7708\n",
      "Epoch 173/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4705 - acc: 0.7795 - val_loss: 0.4892 - val_acc: 0.7708\n",
      "Epoch 174/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4704 - acc: 0.7795 - val_loss: 0.4891 - val_acc: 0.7708\n",
      "Epoch 175/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4704 - acc: 0.7795 - val_loss: 0.4891 - val_acc: 0.7708\n",
      "Epoch 176/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4704 - acc: 0.7795 - val_loss: 0.4891 - val_acc: 0.7708\n",
      "Epoch 177/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4703 - acc: 0.7795 - val_loss: 0.4891 - val_acc: 0.7708\n",
      "Epoch 178/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4702 - acc: 0.7812 - val_loss: 0.4890 - val_acc: 0.7708\n",
      "Epoch 179/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 62us/step - loss: 0.4702 - acc: 0.7795 - val_loss: 0.4890 - val_acc: 0.7708\n",
      "Epoch 180/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4701 - acc: 0.7795 - val_loss: 0.4890 - val_acc: 0.7708\n",
      "Epoch 181/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4701 - acc: 0.7812 - val_loss: 0.4889 - val_acc: 0.7708\n",
      "Epoch 182/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4700 - acc: 0.7795 - val_loss: 0.4889 - val_acc: 0.7708\n",
      "Epoch 183/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4700 - acc: 0.7812 - val_loss: 0.4889 - val_acc: 0.7708\n",
      "Epoch 184/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4699 - acc: 0.7812 - val_loss: 0.4889 - val_acc: 0.7708\n",
      "Epoch 185/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4699 - acc: 0.7812 - val_loss: 0.4888 - val_acc: 0.7708\n",
      "Epoch 186/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4698 - acc: 0.7812 - val_loss: 0.4888 - val_acc: 0.7708\n",
      "Epoch 187/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4698 - acc: 0.7812 - val_loss: 0.4888 - val_acc: 0.7708\n",
      "Epoch 188/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4697 - acc: 0.7812 - val_loss: 0.4887 - val_acc: 0.7708\n",
      "Epoch 189/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4697 - acc: 0.7812 - val_loss: 0.4887 - val_acc: 0.7708\n",
      "Epoch 190/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4696 - acc: 0.7812 - val_loss: 0.4887 - val_acc: 0.7708\n",
      "Epoch 191/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4695 - acc: 0.7812 - val_loss: 0.4887 - val_acc: 0.7708\n",
      "Epoch 192/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4695 - acc: 0.7812 - val_loss: 0.4886 - val_acc: 0.7708\n",
      "Epoch 193/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4694 - acc: 0.7812 - val_loss: 0.4886 - val_acc: 0.7708\n",
      "Epoch 194/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4694 - acc: 0.7812 - val_loss: 0.4886 - val_acc: 0.7708\n",
      "Epoch 195/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4693 - acc: 0.7812 - val_loss: 0.4886 - val_acc: 0.7656\n",
      "Epoch 196/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4693 - acc: 0.7812 - val_loss: 0.4885 - val_acc: 0.7656\n",
      "Epoch 197/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4692 - acc: 0.7812 - val_loss: 0.4885 - val_acc: 0.7656\n",
      "Epoch 198/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4692 - acc: 0.7812 - val_loss: 0.4885 - val_acc: 0.7656\n",
      "Epoch 199/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4692 - acc: 0.7812 - val_loss: 0.4885 - val_acc: 0.7656\n",
      "Epoch 200/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4691 - acc: 0.7812 - val_loss: 0.4884 - val_acc: 0.7656\n",
      "Epoch 201/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4690 - acc: 0.7812 - val_loss: 0.4884 - val_acc: 0.7656\n",
      "Epoch 202/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4690 - acc: 0.7812 - val_loss: 0.4884 - val_acc: 0.7656\n",
      "Epoch 203/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4690 - acc: 0.7812 - val_loss: 0.4884 - val_acc: 0.7656\n",
      "Epoch 204/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4689 - acc: 0.7812 - val_loss: 0.4883 - val_acc: 0.7656\n",
      "Epoch 205/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4689 - acc: 0.7812 - val_loss: 0.4883 - val_acc: 0.7656\n",
      "Epoch 206/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4688 - acc: 0.7812 - val_loss: 0.4883 - val_acc: 0.7656\n",
      "Epoch 207/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4688 - acc: 0.7812 - val_loss: 0.4883 - val_acc: 0.7656\n",
      "Epoch 208/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4687 - acc: 0.7812 - val_loss: 0.4882 - val_acc: 0.7656\n",
      "Epoch 209/1000\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4687 - acc: 0.7812 - val_loss: 0.4882 - val_acc: 0.7656\n",
      "Epoch 210/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4686 - acc: 0.7812 - val_loss: 0.4882 - val_acc: 0.7656\n",
      "Epoch 211/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4686 - acc: 0.7812 - val_loss: 0.4882 - val_acc: 0.7656\n",
      "Epoch 212/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4686 - acc: 0.7812 - val_loss: 0.4881 - val_acc: 0.7656\n",
      "Epoch 213/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4685 - acc: 0.7812 - val_loss: 0.4881 - val_acc: 0.7656\n",
      "Epoch 214/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4684 - acc: 0.7812 - val_loss: 0.4881 - val_acc: 0.7656\n",
      "Epoch 215/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4684 - acc: 0.7830 - val_loss: 0.4881 - val_acc: 0.7656\n",
      "Epoch 216/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4683 - acc: 0.7812 - val_loss: 0.4881 - val_acc: 0.7656\n",
      "Epoch 217/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4683 - acc: 0.7830 - val_loss: 0.4880 - val_acc: 0.7656\n",
      "Epoch 218/1000\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4682 - acc: 0.7830 - val_loss: 0.4880 - val_acc: 0.7656\n",
      "Epoch 219/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4682 - acc: 0.7830 - val_loss: 0.4880 - val_acc: 0.7656\n",
      "Epoch 220/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4682 - acc: 0.7830 - val_loss: 0.4880 - val_acc: 0.7656\n",
      "Epoch 221/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4681 - acc: 0.7830 - val_loss: 0.4879 - val_acc: 0.7656\n",
      "Epoch 222/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4681 - acc: 0.7830 - val_loss: 0.4879 - val_acc: 0.7656\n",
      "Epoch 223/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4680 - acc: 0.7830 - val_loss: 0.4879 - val_acc: 0.7656\n",
      "Epoch 224/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4680 - acc: 0.7830 - val_loss: 0.4879 - val_acc: 0.7656\n",
      "Epoch 225/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4680 - acc: 0.7830 - val_loss: 0.4879 - val_acc: 0.7656\n",
      "Epoch 226/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4679 - acc: 0.7830 - val_loss: 0.4878 - val_acc: 0.7656\n",
      "Epoch 227/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4679 - acc: 0.7830 - val_loss: 0.4878 - val_acc: 0.7656\n",
      "Epoch 228/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4678 - acc: 0.7830 - val_loss: 0.4878 - val_acc: 0.7656\n",
      "Epoch 229/1000\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4678 - acc: 0.7830 - val_loss: 0.4878 - val_acc: 0.7656\n",
      "Epoch 230/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4677 - acc: 0.7830 - val_loss: 0.4878 - val_acc: 0.7656\n",
      "Epoch 231/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4677 - acc: 0.7830 - val_loss: 0.4877 - val_acc: 0.7656\n",
      "Epoch 232/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4676 - acc: 0.7830 - val_loss: 0.4877 - val_acc: 0.7656\n",
      "Epoch 233/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4676 - acc: 0.7830 - val_loss: 0.4877 - val_acc: 0.7656\n",
      "Epoch 234/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4676 - acc: 0.7830 - val_loss: 0.4877 - val_acc: 0.7656\n",
      "Epoch 235/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4675 - acc: 0.7830 - val_loss: 0.4877 - val_acc: 0.7656\n",
      "Epoch 236/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4675 - acc: 0.7830 - val_loss: 0.4876 - val_acc: 0.7656\n",
      "Epoch 237/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4674 - acc: 0.7830 - val_loss: 0.4876 - val_acc: 0.7656\n",
      "Epoch 238/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4674 - acc: 0.7830 - val_loss: 0.4876 - val_acc: 0.7656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 239/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4673 - acc: 0.7830 - val_loss: 0.4876 - val_acc: 0.7656\n",
      "Epoch 240/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4673 - acc: 0.7830 - val_loss: 0.4876 - val_acc: 0.7656\n",
      "Epoch 241/1000\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4673 - acc: 0.7830 - val_loss: 0.4876 - val_acc: 0.7656\n",
      "Epoch 242/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4672 - acc: 0.7830 - val_loss: 0.4875 - val_acc: 0.7656\n",
      "Epoch 243/1000\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4672 - acc: 0.7830 - val_loss: 0.4875 - val_acc: 0.7656\n",
      "Epoch 244/1000\n",
      "576/576 [==============================] - 0s 101us/step - loss: 0.4671 - acc: 0.7830 - val_loss: 0.4875 - val_acc: 0.7656\n",
      "Epoch 245/1000\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.4671 - acc: 0.7830 - val_loss: 0.4875 - val_acc: 0.7656\n",
      "Epoch 246/1000\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4671 - acc: 0.7830 - val_loss: 0.4875 - val_acc: 0.7656\n",
      "Epoch 247/1000\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4670 - acc: 0.7830 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 248/1000\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4670 - acc: 0.7847 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 249/1000\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4669 - acc: 0.7830 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 250/1000\n",
      "576/576 [==============================] - 0s 97us/step - loss: 0.4669 - acc: 0.7830 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 251/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4669 - acc: 0.7847 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 252/1000\n",
      "576/576 [==============================] - 0s 110us/step - loss: 0.4668 - acc: 0.7847 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 253/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4668 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7656\n",
      "Epoch 254/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4668 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7656\n",
      "Epoch 255/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4667 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7656\n",
      "Epoch 256/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4667 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7656\n",
      "Epoch 257/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4666 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7656\n",
      "Epoch 258/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4666 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7656\n",
      "Epoch 259/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4665 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7656\n",
      "Epoch 260/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4665 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7656\n",
      "Epoch 261/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4665 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7656\n",
      "Epoch 262/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4664 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7656\n",
      "Epoch 263/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4664 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7656\n",
      "Epoch 264/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4663 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7656\n",
      "Epoch 265/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4663 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7656\n",
      "Epoch 266/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4663 - acc: 0.7847 - val_loss: 0.4871 - val_acc: 0.7656\n",
      "Epoch 267/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4662 - acc: 0.7847 - val_loss: 0.4871 - val_acc: 0.7656\n",
      "Epoch 268/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4662 - acc: 0.7847 - val_loss: 0.4871 - val_acc: 0.7656\n",
      "Epoch 269/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4662 - acc: 0.7847 - val_loss: 0.4871 - val_acc: 0.7656\n",
      "Epoch 270/1000\n",
      "576/576 [==============================] - 0s 98us/step - loss: 0.4661 - acc: 0.7847 - val_loss: 0.4871 - val_acc: 0.7656\n",
      "Epoch 271/1000\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.4661 - acc: 0.7847 - val_loss: 0.4871 - val_acc: 0.7656\n",
      "Epoch 272/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4661 - acc: 0.7847 - val_loss: 0.4870 - val_acc: 0.7656\n",
      "Epoch 273/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4660 - acc: 0.7847 - val_loss: 0.4870 - val_acc: 0.7656\n",
      "Epoch 274/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4660 - acc: 0.7847 - val_loss: 0.4870 - val_acc: 0.7656\n",
      "Epoch 275/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4660 - acc: 0.7847 - val_loss: 0.4870 - val_acc: 0.7656\n",
      "Epoch 276/1000\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4659 - acc: 0.7847 - val_loss: 0.4870 - val_acc: 0.7656\n",
      "Epoch 277/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4659 - acc: 0.7847 - val_loss: 0.4870 - val_acc: 0.7656\n",
      "Epoch 278/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4454 - acc: 0.781 - 0s 76us/step - loss: 0.4658 - acc: 0.7865 - val_loss: 0.4870 - val_acc: 0.7656\n",
      "Epoch 279/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4658 - acc: 0.7847 - val_loss: 0.4870 - val_acc: 0.7656\n",
      "Epoch 280/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4658 - acc: 0.7847 - val_loss: 0.4869 - val_acc: 0.7656\n",
      "Epoch 281/1000\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4657 - acc: 0.7847 - val_loss: 0.4869 - val_acc: 0.7656\n",
      "Epoch 282/1000\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4657 - acc: 0.7830 - val_loss: 0.4869 - val_acc: 0.7656\n",
      "Epoch 283/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4657 - acc: 0.7847 - val_loss: 0.4869 - val_acc: 0.7656\n",
      "Epoch 284/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4656 - acc: 0.7847 - val_loss: 0.4869 - val_acc: 0.7656\n",
      "Epoch 285/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4656 - acc: 0.7847 - val_loss: 0.4869 - val_acc: 0.7656\n",
      "Epoch 286/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4656 - acc: 0.7847 - val_loss: 0.4869 - val_acc: 0.7656\n",
      "Epoch 287/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4655 - acc: 0.7847 - val_loss: 0.4868 - val_acc: 0.7656\n",
      "Epoch 288/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4655 - acc: 0.7847 - val_loss: 0.4868 - val_acc: 0.7656\n",
      "Epoch 289/1000\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4655 - acc: 0.7847 - val_loss: 0.4868 - val_acc: 0.7656\n",
      "Epoch 290/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4654 - acc: 0.7847 - val_loss: 0.4868 - val_acc: 0.7656\n",
      "Epoch 291/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4654 - acc: 0.7847 - val_loss: 0.4868 - val_acc: 0.7656\n",
      "Epoch 292/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.5255 - acc: 0.687 - 0s 61us/step - loss: 0.4654 - acc: 0.7847 - val_loss: 0.4868 - val_acc: 0.7656\n",
      "Epoch 293/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4653 - acc: 0.7847 - val_loss: 0.4868 - val_acc: 0.7656\n",
      "Epoch 294/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4653 - acc: 0.7847 - val_loss: 0.4868 - val_acc: 0.7656\n",
      "Epoch 295/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4653 - acc: 0.7847 - val_loss: 0.4867 - val_acc: 0.7656\n",
      "Epoch 296/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4652 - acc: 0.7847 - val_loss: 0.4867 - val_acc: 0.7656\n",
      "Epoch 297/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4652 - acc: 0.7847 - val_loss: 0.4867 - val_acc: 0.7656\n",
      "Epoch 298/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 68us/step - loss: 0.4652 - acc: 0.7847 - val_loss: 0.4867 - val_acc: 0.7656\n",
      "Epoch 299/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4651 - acc: 0.7847 - val_loss: 0.4867 - val_acc: 0.7656\n",
      "Epoch 300/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4651 - acc: 0.7847 - val_loss: 0.4867 - val_acc: 0.7656\n",
      "Epoch 301/1000\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4651 - acc: 0.7847 - val_loss: 0.4867 - val_acc: 0.7656\n",
      "Epoch 302/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4650 - acc: 0.7847 - val_loss: 0.4867 - val_acc: 0.7656\n",
      "Epoch 303/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4650 - acc: 0.7847 - val_loss: 0.4867 - val_acc: 0.7656\n",
      "Epoch 304/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4650 - acc: 0.7847 - val_loss: 0.4866 - val_acc: 0.7656\n",
      "Epoch 305/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4649 - acc: 0.7847 - val_loss: 0.4866 - val_acc: 0.7656\n",
      "Epoch 306/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4188 - acc: 0.812 - 0s 63us/step - loss: 0.4649 - acc: 0.7847 - val_loss: 0.4866 - val_acc: 0.7656\n",
      "Epoch 307/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4649 - acc: 0.7847 - val_loss: 0.4866 - val_acc: 0.7656\n",
      "Epoch 308/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4648 - acc: 0.7847 - val_loss: 0.4866 - val_acc: 0.7656\n",
      "Epoch 309/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4648 - acc: 0.7847 - val_loss: 0.4866 - val_acc: 0.7656\n",
      "Epoch 310/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4648 - acc: 0.7865 - val_loss: 0.4866 - val_acc: 0.7656\n",
      "Epoch 311/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4647 - acc: 0.7847 - val_loss: 0.4866 - val_acc: 0.7656\n",
      "Epoch 312/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4647 - acc: 0.7865 - val_loss: 0.4866 - val_acc: 0.7656\n",
      "Epoch 313/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4647 - acc: 0.7865 - val_loss: 0.4866 - val_acc: 0.7656\n",
      "Epoch 314/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4647 - acc: 0.7865 - val_loss: 0.4865 - val_acc: 0.7656\n",
      "Epoch 315/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4646 - acc: 0.7847 - val_loss: 0.4865 - val_acc: 0.7656\n",
      "Epoch 316/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4646 - acc: 0.7865 - val_loss: 0.4865 - val_acc: 0.7656\n",
      "Epoch 317/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4646 - acc: 0.7865 - val_loss: 0.4865 - val_acc: 0.7656\n",
      "Epoch 318/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4645 - acc: 0.7865 - val_loss: 0.4865 - val_acc: 0.7656\n",
      "Epoch 319/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4645 - acc: 0.7865 - val_loss: 0.4865 - val_acc: 0.7656\n",
      "Epoch 320/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4645 - acc: 0.7865 - val_loss: 0.4865 - val_acc: 0.7656\n",
      "Epoch 321/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4644 - acc: 0.7865 - val_loss: 0.4865 - val_acc: 0.7656\n",
      "Epoch 322/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4644 - acc: 0.7865 - val_loss: 0.4865 - val_acc: 0.7656\n",
      "Epoch 323/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4644 - acc: 0.7865 - val_loss: 0.4865 - val_acc: 0.7656\n",
      "Epoch 324/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4643 - acc: 0.7865 - val_loss: 0.4865 - val_acc: 0.7656\n",
      "Epoch 325/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4643 - acc: 0.7865 - val_loss: 0.4864 - val_acc: 0.7656\n",
      "Epoch 326/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4643 - acc: 0.7865 - val_loss: 0.4864 - val_acc: 0.7656\n",
      "Epoch 327/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4643 - acc: 0.7865 - val_loss: 0.4864 - val_acc: 0.7656\n",
      "Epoch 328/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4642 - acc: 0.7865 - val_loss: 0.4864 - val_acc: 0.7656\n",
      "Epoch 329/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4642 - acc: 0.7865 - val_loss: 0.4864 - val_acc: 0.7656\n",
      "Epoch 330/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4642 - acc: 0.7865 - val_loss: 0.4864 - val_acc: 0.7656\n",
      "Epoch 331/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4642 - acc: 0.7865 - val_loss: 0.4864 - val_acc: 0.7656\n",
      "Epoch 332/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4641 - acc: 0.7865 - val_loss: 0.4864 - val_acc: 0.7656\n",
      "Epoch 333/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4641 - acc: 0.7865 - val_loss: 0.4864 - val_acc: 0.7656\n",
      "Epoch 334/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4641 - acc: 0.7865 - val_loss: 0.4864 - val_acc: 0.7656\n",
      "Epoch 335/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4640 - acc: 0.7865 - val_loss: 0.4864 - val_acc: 0.7656\n",
      "Epoch 336/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4640 - acc: 0.7865 - val_loss: 0.4863 - val_acc: 0.7656\n",
      "Epoch 337/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4640 - acc: 0.7865 - val_loss: 0.4863 - val_acc: 0.7656\n",
      "Epoch 338/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4640 - acc: 0.7865 - val_loss: 0.4863 - val_acc: 0.7656\n",
      "Epoch 339/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4639 - acc: 0.7865 - val_loss: 0.4863 - val_acc: 0.7656\n",
      "Epoch 340/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4639 - acc: 0.7865 - val_loss: 0.4863 - val_acc: 0.7656\n",
      "Epoch 341/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4639 - acc: 0.7865 - val_loss: 0.4863 - val_acc: 0.7656\n",
      "Epoch 342/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4638 - acc: 0.7899 - val_loss: 0.4863 - val_acc: 0.7656\n",
      "Epoch 343/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4638 - acc: 0.7899 - val_loss: 0.4863 - val_acc: 0.7656\n",
      "Epoch 344/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4638 - acc: 0.7882 - val_loss: 0.4863 - val_acc: 0.7656\n",
      "Epoch 345/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4637 - acc: 0.7882 - val_loss: 0.4863 - val_acc: 0.7656\n",
      "Epoch 346/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4637 - acc: 0.7899 - val_loss: 0.4863 - val_acc: 0.7656\n",
      "Epoch 347/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4637 - acc: 0.7882 - val_loss: 0.4863 - val_acc: 0.7656\n",
      "Epoch 348/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4637 - acc: 0.7899 - val_loss: 0.4863 - val_acc: 0.7656\n",
      "Epoch 349/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4637 - acc: 0.7899 - val_loss: 0.4863 - val_acc: 0.7656\n",
      "Epoch 350/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4636 - acc: 0.7899 - val_loss: 0.4862 - val_acc: 0.7656\n",
      "Epoch 351/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4636 - acc: 0.7899 - val_loss: 0.4862 - val_acc: 0.7656\n",
      "Epoch 352/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4636 - acc: 0.7899 - val_loss: 0.4862 - val_acc: 0.7656\n",
      "Epoch 353/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4635 - acc: 0.7899 - val_loss: 0.4862 - val_acc: 0.7656\n",
      "Epoch 354/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4635 - acc: 0.7917 - val_loss: 0.4862 - val_acc: 0.7656\n",
      "Epoch 355/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4635 - acc: 0.7899 - val_loss: 0.4862 - val_acc: 0.7656\n",
      "Epoch 356/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4635 - acc: 0.7917 - val_loss: 0.4862 - val_acc: 0.7656\n",
      "Epoch 357/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 59us/step - loss: 0.4634 - acc: 0.7917 - val_loss: 0.4862 - val_acc: 0.7656\n",
      "Epoch 358/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4634 - acc: 0.7917 - val_loss: 0.4862 - val_acc: 0.7656\n",
      "Epoch 359/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4634 - acc: 0.7917 - val_loss: 0.4862 - val_acc: 0.7656\n",
      "Epoch 360/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4634 - acc: 0.7917 - val_loss: 0.4862 - val_acc: 0.7656\n",
      "Epoch 361/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4633 - acc: 0.7917 - val_loss: 0.4862 - val_acc: 0.7656\n",
      "Epoch 362/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4633 - acc: 0.7917 - val_loss: 0.4862 - val_acc: 0.7656\n",
      "Epoch 363/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4633 - acc: 0.7917 - val_loss: 0.4862 - val_acc: 0.7656\n",
      "Epoch 364/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4632 - acc: 0.7917 - val_loss: 0.4862 - val_acc: 0.7656\n",
      "Epoch 365/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4632 - acc: 0.7917 - val_loss: 0.4861 - val_acc: 0.7656\n",
      "Epoch 366/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4632 - acc: 0.7917 - val_loss: 0.4861 - val_acc: 0.7656\n",
      "Epoch 367/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4632 - acc: 0.7917 - val_loss: 0.4861 - val_acc: 0.7656\n",
      "Epoch 368/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4632 - acc: 0.7899 - val_loss: 0.4861 - val_acc: 0.7656\n",
      "Epoch 369/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4631 - acc: 0.7917 - val_loss: 0.4861 - val_acc: 0.7656\n",
      "Epoch 370/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4631 - acc: 0.7917 - val_loss: 0.4861 - val_acc: 0.7656\n",
      "Epoch 371/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4631 - acc: 0.7917 - val_loss: 0.4861 - val_acc: 0.7656\n",
      "Epoch 372/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4630 - acc: 0.7917 - val_loss: 0.4861 - val_acc: 0.7656\n",
      "Epoch 373/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4630 - acc: 0.7899 - val_loss: 0.4861 - val_acc: 0.7656\n",
      "Epoch 374/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4630 - acc: 0.7917 - val_loss: 0.4861 - val_acc: 0.7656\n",
      "Epoch 375/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4630 - acc: 0.7899 - val_loss: 0.4861 - val_acc: 0.7656\n",
      "Epoch 376/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4630 - acc: 0.7917 - val_loss: 0.4861 - val_acc: 0.7656\n",
      "Epoch 377/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4629 - acc: 0.7899 - val_loss: 0.4861 - val_acc: 0.7656\n",
      "Epoch 378/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4629 - acc: 0.7899 - val_loss: 0.4861 - val_acc: 0.7656\n",
      "Epoch 379/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4629 - acc: 0.7899 - val_loss: 0.4861 - val_acc: 0.7656\n",
      "Epoch 380/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4629 - acc: 0.7917 - val_loss: 0.4861 - val_acc: 0.7656\n",
      "Epoch 381/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4628 - acc: 0.7899 - val_loss: 0.4861 - val_acc: 0.7656\n",
      "Epoch 382/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4628 - acc: 0.7899 - val_loss: 0.4861 - val_acc: 0.7656\n",
      "Epoch 383/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4628 - acc: 0.7899 - val_loss: 0.4861 - val_acc: 0.7656\n",
      "Epoch 384/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4628 - acc: 0.7899 - val_loss: 0.4861 - val_acc: 0.7656\n",
      "Epoch 385/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4627 - acc: 0.7899 - val_loss: 0.4860 - val_acc: 0.7656\n",
      "Epoch 386/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4627 - acc: 0.7899 - val_loss: 0.4860 - val_acc: 0.7656\n",
      "Epoch 387/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4627 - acc: 0.7899 - val_loss: 0.4860 - val_acc: 0.7656\n",
      "Epoch 388/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4627 - acc: 0.7899 - val_loss: 0.4860 - val_acc: 0.7656\n",
      "Epoch 389/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4627 - acc: 0.7899 - val_loss: 0.4860 - val_acc: 0.7656\n",
      "Epoch 390/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4626 - acc: 0.7899 - val_loss: 0.4860 - val_acc: 0.7656\n",
      "Epoch 391/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4626 - acc: 0.7899 - val_loss: 0.4860 - val_acc: 0.7656\n",
      "Epoch 392/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4626 - acc: 0.7899 - val_loss: 0.4860 - val_acc: 0.7656\n",
      "Epoch 393/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4625 - acc: 0.7899 - val_loss: 0.4860 - val_acc: 0.7656\n",
      "Epoch 394/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4625 - acc: 0.7899 - val_loss: 0.4860 - val_acc: 0.7656\n",
      "Epoch 395/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4625 - acc: 0.7899 - val_loss: 0.4860 - val_acc: 0.7656\n",
      "Epoch 396/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4625 - acc: 0.7899 - val_loss: 0.4860 - val_acc: 0.7656\n",
      "Epoch 397/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4625 - acc: 0.7899 - val_loss: 0.4860 - val_acc: 0.7656\n",
      "Epoch 398/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4625 - acc: 0.7899 - val_loss: 0.4860 - val_acc: 0.7656\n",
      "Epoch 399/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4624 - acc: 0.7899 - val_loss: 0.4860 - val_acc: 0.7656\n",
      "Epoch 400/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4624 - acc: 0.7899 - val_loss: 0.4860 - val_acc: 0.7656\n",
      "Epoch 401/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4624 - acc: 0.7899 - val_loss: 0.4860 - val_acc: 0.7656\n",
      "Epoch 402/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4623 - acc: 0.7899 - val_loss: 0.4860 - val_acc: 0.7656\n",
      "Epoch 403/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4623 - acc: 0.7899 - val_loss: 0.4860 - val_acc: 0.7656\n",
      "Epoch 404/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4623 - acc: 0.7899 - val_loss: 0.4860 - val_acc: 0.7656\n",
      "Epoch 405/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4623 - acc: 0.7899 - val_loss: 0.4860 - val_acc: 0.7656\n",
      "Epoch 406/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4623 - acc: 0.7899 - val_loss: 0.4860 - val_acc: 0.7656\n",
      "Epoch 407/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4622 - acc: 0.7899 - val_loss: 0.4860 - val_acc: 0.7656\n",
      "Epoch 408/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4622 - acc: 0.7899 - val_loss: 0.4860 - val_acc: 0.7656\n",
      "Epoch 409/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4622 - acc: 0.7899 - val_loss: 0.4860 - val_acc: 0.7656\n",
      "Epoch 410/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4622 - acc: 0.7899 - val_loss: 0.4860 - val_acc: 0.7656\n",
      "Epoch 411/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4622 - acc: 0.7882 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 412/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4621 - acc: 0.7899 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 413/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4621 - acc: 0.7899 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 414/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4621 - acc: 0.7899 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 415/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4621 - acc: 0.7882 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 416/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4621 - acc: 0.7882 - val_loss: 0.4859 - val_acc: 0.7656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 417/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4620 - acc: 0.7882 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 418/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4620 - acc: 0.7882 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 419/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4620 - acc: 0.7882 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 420/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4620 - acc: 0.7882 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 421/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4620 - acc: 0.7882 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 422/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4619 - acc: 0.7882 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 423/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4619 - acc: 0.7882 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 424/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4619 - acc: 0.7882 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 425/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4619 - acc: 0.7882 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 426/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4618 - acc: 0.7882 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 427/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4618 - acc: 0.7882 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 428/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4618 - acc: 0.7882 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 429/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4618 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 430/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4618 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 431/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4617 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 432/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4617 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 433/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4617 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 434/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4617 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 435/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4617 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 436/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4616 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 437/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4616 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 438/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4616 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 439/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4616 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 440/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4616 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 441/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4615 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 442/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4615 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 443/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4615 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 444/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4615 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 445/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4615 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7656\n",
      "Epoch 446/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4614 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 447/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4614 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 448/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4614 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 449/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4614 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 450/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4614 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 451/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4614 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 452/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4613 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 453/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4613 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 454/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4613 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 455/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4613 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 456/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4613 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 457/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4612 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 458/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4612 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 459/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4612 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 460/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4612 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 461/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4612 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 462/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4612 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 463/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4611 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 464/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4611 - acc: 0.7865 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 465/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4611 - acc: 0.7865 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 466/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4611 - acc: 0.7865 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 467/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4611 - acc: 0.7865 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 468/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4610 - acc: 0.7865 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 469/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4610 - acc: 0.7865 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 470/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4610 - acc: 0.7865 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 471/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4610 - acc: 0.7865 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 472/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4610 - acc: 0.7865 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 473/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4609 - acc: 0.7865 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 474/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4609 - acc: 0.7865 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 475/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4609 - acc: 0.7865 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 476/1000\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4609 - acc: 0.7865 - val_loss: 0.4858 - val_acc: 0.7604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 477/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4609 - acc: 0.7865 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 478/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4609 - acc: 0.7865 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 479/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4608 - acc: 0.7865 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 480/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4608 - acc: 0.7865 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 481/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4608 - acc: 0.7865 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 482/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4608 - acc: 0.7865 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 483/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4608 - acc: 0.7865 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 484/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4608 - acc: 0.7865 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 485/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4607 - acc: 0.7847 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 486/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4607 - acc: 0.7847 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 487/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4607 - acc: 0.7865 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 488/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4607 - acc: 0.7865 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 489/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4607 - acc: 0.7865 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 490/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4607 - acc: 0.7847 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 491/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4607 - acc: 0.7865 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 492/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4606 - acc: 0.7847 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 493/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4606 - acc: 0.7847 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 494/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4606 - acc: 0.7847 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 495/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4606 - acc: 0.7847 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 496/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4606 - acc: 0.7847 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 497/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4606 - acc: 0.7847 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 498/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4606 - acc: 0.7847 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 499/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4605 - acc: 0.7847 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 500/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4605 - acc: 0.7847 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 501/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4605 - acc: 0.7847 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 502/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4605 - acc: 0.7847 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 503/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4604 - acc: 0.7847 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 504/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4604 - acc: 0.7847 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 505/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4605 - acc: 0.7847 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 506/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4604 - acc: 0.7847 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 507/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4604 - acc: 0.7847 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 508/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4604 - acc: 0.7847 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 509/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4604 - acc: 0.7847 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 510/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4603 - acc: 0.7847 - val_loss: 0.4858 - val_acc: 0.7604\n",
      "Epoch 511/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4603 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 512/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4603 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 513/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4603 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 514/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4603 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 515/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4603 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 516/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4603 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 517/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4602 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 518/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4602 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 519/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4602 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 520/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4602 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 521/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4602 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 522/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4602 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 523/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4601 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 524/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4601 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 525/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4601 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 526/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4601 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 527/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4601 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 528/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4601 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 529/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4601 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 530/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4601 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 531/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4600 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 532/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4600 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 533/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4600 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 534/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4600 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 535/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4600 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 536/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4600 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 537/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4600 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 538/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4599 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 539/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4599 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 540/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4599 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 541/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4599 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 542/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4599 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 543/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4599 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 544/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4599 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 545/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4598 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 546/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4598 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 547/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4598 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 548/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4598 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 549/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4598 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 550/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4598 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 551/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4597 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 552/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4597 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 553/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4597 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 554/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4597 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 555/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4597 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 556/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4597 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 557/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4597 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 558/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4597 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 559/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4596 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 560/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4596 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 561/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4596 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 562/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4596 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 563/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4596 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 564/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4596 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 565/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4596 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 566/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4596 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 567/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4595 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 568/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4595 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 569/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4595 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 570/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4595 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 571/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4595 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 572/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4595 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 573/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4595 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 574/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4594 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 575/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4594 - acc: 0.7847 - val_loss: 0.4859 - val_acc: 0.7604\n",
      "Epoch 576/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4594 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7604\n",
      "Epoch 577/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4594 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7604\n",
      "Epoch 578/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4594 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7604\n",
      "Epoch 579/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4594 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7604\n",
      "Epoch 580/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4594 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7604\n",
      "Epoch 581/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4594 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7604\n",
      "Epoch 582/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4593 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7604\n",
      "Epoch 583/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4593 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7604\n",
      "Epoch 584/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4593 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7604\n",
      "Epoch 585/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4593 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7604\n",
      "Epoch 586/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4593 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7604\n",
      "Epoch 587/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4593 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 588/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4593 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 589/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4593 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 590/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4593 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 591/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4592 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 592/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4592 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 593/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4592 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 594/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4592 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 595/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4592 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 596/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4592 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 597/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4592 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 598/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4592 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 599/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4591 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 600/1000\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4591 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 601/1000\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4591 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 602/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4591 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 603/1000\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4591 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 604/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4591 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 605/1000\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4591 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 606/1000\n",
      "576/576 [==============================] - 0s 99us/step - loss: 0.4591 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 607/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4591 - acc: 0.7847 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 608/1000\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4590 - acc: 0.7830 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 609/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4590 - acc: 0.7830 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 610/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4590 - acc: 0.7830 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 611/1000\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4590 - acc: 0.7830 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 612/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4590 - acc: 0.7830 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 613/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4590 - acc: 0.7830 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 614/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4590 - acc: 0.7830 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 615/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4590 - acc: 0.7830 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 616/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4589 - acc: 0.7812 - val_loss: 0.4860 - val_acc: 0.7552\n",
      "Epoch 617/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4589 - acc: 0.7830 - val_loss: 0.4861 - val_acc: 0.7552\n",
      "Epoch 618/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4589 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7552\n",
      "Epoch 619/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4589 - acc: 0.7830 - val_loss: 0.4861 - val_acc: 0.7552\n",
      "Epoch 620/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4589 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7552\n",
      "Epoch 621/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4589 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7500\n",
      "Epoch 622/1000\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.4589 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7500\n",
      "Epoch 623/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4589 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7500\n",
      "Epoch 624/1000\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4589 - acc: 0.7830 - val_loss: 0.4861 - val_acc: 0.7500\n",
      "Epoch 625/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4588 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7500\n",
      "Epoch 626/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4589 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7500\n",
      "Epoch 627/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4589 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7500\n",
      "Epoch 628/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4588 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7500\n",
      "Epoch 629/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4588 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7500\n",
      "Epoch 630/1000\n",
      "576/576 [==============================] - 0s 99us/step - loss: 0.4588 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7500\n",
      "Epoch 631/1000\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.4588 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7500\n",
      "Epoch 632/1000\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.4588 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7500\n",
      "Epoch 633/1000\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.4588 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7500\n",
      "Epoch 634/1000\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4587 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7500\n",
      "Epoch 635/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4587 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7500\n",
      "Epoch 636/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4587 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7500\n",
      "Epoch 637/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4587 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7500\n",
      "Epoch 638/1000\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4587 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7500\n",
      "Epoch 639/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4587 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7500\n",
      "Epoch 640/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4587 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7552\n",
      "Epoch 641/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4587 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7552\n",
      "Epoch 642/1000\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4587 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7552\n",
      "Epoch 643/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4587 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7552\n",
      "Epoch 644/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4586 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7552\n",
      "Epoch 645/1000\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4586 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7552\n",
      "Epoch 646/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4586 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7552\n",
      "Epoch 647/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4586 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7552\n",
      "Epoch 648/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4586 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7552\n",
      "Epoch 649/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4586 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7552\n",
      "Epoch 650/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4586 - acc: 0.7812 - val_loss: 0.4861 - val_acc: 0.7552\n",
      "Epoch 651/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4586 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 652/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4586 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 653/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4586 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 654/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4585 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 655/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4585 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 656/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4585 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 657/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4585 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 658/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4585 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 659/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4585 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 660/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4585 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 661/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4585 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 662/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4585 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 663/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4584 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 664/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4584 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 665/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4584 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 666/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4584 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 667/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4584 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 668/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4584 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 669/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4584 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 670/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4584 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 671/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4584 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 672/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4584 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 673/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4584 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 674/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4583 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 675/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4583 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 676/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4583 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 677/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4583 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 678/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4583 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 679/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4583 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 680/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4583 - acc: 0.7812 - val_loss: 0.4862 - val_acc: 0.7552\n",
      "Epoch 681/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4583 - acc: 0.7812 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 682/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4583 - acc: 0.7812 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 683/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4583 - acc: 0.7812 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 684/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4583 - acc: 0.7812 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 685/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4582 - acc: 0.7812 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 686/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4582 - acc: 0.7812 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 687/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4582 - acc: 0.7812 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 688/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4582 - acc: 0.7812 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 689/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4582 - acc: 0.7812 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 690/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4582 - acc: 0.7812 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 691/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.3733 - acc: 0.812 - 0s 63us/step - loss: 0.4582 - acc: 0.7812 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 692/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4773 - acc: 0.750 - 0s 64us/step - loss: 0.4582 - acc: 0.7812 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 693/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4582 - acc: 0.7812 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 694/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4581 - acc: 0.7812 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 695/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4581 - acc: 0.7812 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 696/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4582 - acc: 0.7812 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 697/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4581 - acc: 0.7812 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 698/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4581 - acc: 0.7812 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 699/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.3598 - acc: 0.875 - 0s 70us/step - loss: 0.4581 - acc: 0.7830 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 700/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4581 - acc: 0.7812 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 701/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4581 - acc: 0.7812 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 702/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4581 - acc: 0.7812 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 703/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4581 - acc: 0.7830 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 704/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4581 - acc: 0.7830 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 705/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4581 - acc: 0.7830 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 706/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4581 - acc: 0.7830 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 707/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4580 - acc: 0.7830 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 708/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4580 - acc: 0.7830 - val_loss: 0.4863 - val_acc: 0.7552\n",
      "Epoch 709/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4580 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "Epoch 710/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4580 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "Epoch 711/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4580 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "Epoch 712/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4580 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "Epoch 713/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4580 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "Epoch 714/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4580 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "Epoch 715/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4580 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 716/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4579 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "Epoch 717/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4580 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "Epoch 718/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4579 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "Epoch 719/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4579 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "Epoch 720/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4579 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "Epoch 721/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4579 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "Epoch 722/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4579 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "Epoch 723/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4579 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "Epoch 724/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4579 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "Epoch 725/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.3734 - acc: 0.875 - 0s 66us/step - loss: 0.4579 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "Epoch 726/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4579 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "Epoch 727/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4579 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "Epoch 728/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4579 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "Epoch 729/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4579 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "Epoch 730/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4578 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "Epoch 731/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4578 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "Epoch 732/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4578 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "Epoch 733/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4578 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "Epoch 734/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4578 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "Epoch 735/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4578 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "Epoch 736/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4578 - acc: 0.7830 - val_loss: 0.4864 - val_acc: 0.7552\n",
      "Epoch 737/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4578 - acc: 0.7830 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 738/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4578 - acc: 0.7830 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 739/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4578 - acc: 0.7830 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 740/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4578 - acc: 0.7830 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 741/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4578 - acc: 0.7830 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 742/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4578 - acc: 0.7830 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 743/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4577 - acc: 0.7812 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 744/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4577 - acc: 0.7812 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 745/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4577 - acc: 0.7830 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 746/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4577 - acc: 0.7830 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 747/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4577 - acc: 0.7830 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 748/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4577 - acc: 0.7830 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 749/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4577 - acc: 0.7830 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 750/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4577 - acc: 0.7830 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 751/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4577 - acc: 0.7830 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 752/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4577 - acc: 0.7830 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 753/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4577 - acc: 0.7830 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 754/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4576 - acc: 0.7830 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 755/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4576 - acc: 0.7830 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 756/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4576 - acc: 0.7830 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 757/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4576 - acc: 0.7830 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 758/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4576 - acc: 0.7830 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 759/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4576 - acc: 0.7830 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 760/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4576 - acc: 0.7830 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 761/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4576 - acc: 0.7812 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 762/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4576 - acc: 0.7812 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 763/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4576 - acc: 0.7812 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 764/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4576 - acc: 0.7812 - val_loss: 0.4865 - val_acc: 0.7552\n",
      "Epoch 765/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4576 - acc: 0.7812 - val_loss: 0.4866 - val_acc: 0.7552\n",
      "Epoch 766/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4575 - acc: 0.7812 - val_loss: 0.4866 - val_acc: 0.7552\n",
      "Epoch 767/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4575 - acc: 0.7812 - val_loss: 0.4866 - val_acc: 0.7552\n",
      "Epoch 768/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4575 - acc: 0.7812 - val_loss: 0.4866 - val_acc: 0.7552\n",
      "Epoch 769/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4575 - acc: 0.7812 - val_loss: 0.4866 - val_acc: 0.7552\n",
      "Epoch 770/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4575 - acc: 0.7812 - val_loss: 0.4866 - val_acc: 0.7552\n",
      "Epoch 771/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4575 - acc: 0.7812 - val_loss: 0.4866 - val_acc: 0.7552\n",
      "Epoch 772/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4575 - acc: 0.7812 - val_loss: 0.4866 - val_acc: 0.7552\n",
      "Epoch 773/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4575 - acc: 0.7812 - val_loss: 0.4866 - val_acc: 0.7552\n",
      "Epoch 774/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4575 - acc: 0.7812 - val_loss: 0.4866 - val_acc: 0.7552\n",
      "Epoch 775/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 57us/step - loss: 0.4575 - acc: 0.7812 - val_loss: 0.4866 - val_acc: 0.7552\n",
      "Epoch 776/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4575 - acc: 0.7812 - val_loss: 0.4866 - val_acc: 0.7552\n",
      "Epoch 777/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4575 - acc: 0.7812 - val_loss: 0.4866 - val_acc: 0.7552\n",
      "Epoch 778/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4575 - acc: 0.7812 - val_loss: 0.4866 - val_acc: 0.7552\n",
      "Epoch 779/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4574 - acc: 0.7812 - val_loss: 0.4866 - val_acc: 0.7552\n",
      "Epoch 780/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4575 - acc: 0.7812 - val_loss: 0.4866 - val_acc: 0.7552\n",
      "Epoch 781/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4574 - acc: 0.7812 - val_loss: 0.4866 - val_acc: 0.7552\n",
      "Epoch 782/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4575 - acc: 0.7812 - val_loss: 0.4866 - val_acc: 0.7552\n",
      "Epoch 783/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4574 - acc: 0.7812 - val_loss: 0.4866 - val_acc: 0.7552\n",
      "Epoch 784/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4574 - acc: 0.7812 - val_loss: 0.4866 - val_acc: 0.7552\n",
      "Epoch 785/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4574 - acc: 0.7812 - val_loss: 0.4866 - val_acc: 0.7552\n",
      "Epoch 786/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4574 - acc: 0.7812 - val_loss: 0.4866 - val_acc: 0.7552\n",
      "Epoch 787/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4574 - acc: 0.7812 - val_loss: 0.4866 - val_acc: 0.7552\n",
      "Epoch 788/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4574 - acc: 0.7812 - val_loss: 0.4866 - val_acc: 0.7552\n",
      "Epoch 789/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4574 - acc: 0.7812 - val_loss: 0.4866 - val_acc: 0.7552\n",
      "Epoch 790/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4574 - acc: 0.7812 - val_loss: 0.4867 - val_acc: 0.7552\n",
      "Epoch 791/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4574 - acc: 0.7812 - val_loss: 0.4867 - val_acc: 0.7552\n",
      "Epoch 792/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4573 - acc: 0.7812 - val_loss: 0.4867 - val_acc: 0.7552\n",
      "Epoch 793/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4573 - acc: 0.7812 - val_loss: 0.4867 - val_acc: 0.7552\n",
      "Epoch 794/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4574 - acc: 0.7812 - val_loss: 0.4867 - val_acc: 0.7552\n",
      "Epoch 795/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4573 - acc: 0.7812 - val_loss: 0.4867 - val_acc: 0.7552\n",
      "Epoch 796/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4573 - acc: 0.7812 - val_loss: 0.4867 - val_acc: 0.7552\n",
      "Epoch 797/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4574 - acc: 0.7812 - val_loss: 0.4867 - val_acc: 0.7552\n",
      "Epoch 798/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4573 - acc: 0.7812 - val_loss: 0.4867 - val_acc: 0.7552\n",
      "Epoch 799/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4573 - acc: 0.7812 - val_loss: 0.4867 - val_acc: 0.7552\n",
      "Epoch 800/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4573 - acc: 0.7812 - val_loss: 0.4867 - val_acc: 0.7552\n",
      "Epoch 801/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4573 - acc: 0.7812 - val_loss: 0.4867 - val_acc: 0.7552\n",
      "Epoch 802/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4573 - acc: 0.7812 - val_loss: 0.4867 - val_acc: 0.7552\n",
      "Epoch 803/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4573 - acc: 0.7812 - val_loss: 0.4867 - val_acc: 0.7552\n",
      "Epoch 804/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4573 - acc: 0.7812 - val_loss: 0.4867 - val_acc: 0.7552\n",
      "Epoch 805/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4573 - acc: 0.7812 - val_loss: 0.4867 - val_acc: 0.7552\n",
      "Epoch 806/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4572 - acc: 0.7812 - val_loss: 0.4867 - val_acc: 0.7552\n",
      "Epoch 807/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4572 - acc: 0.7812 - val_loss: 0.4867 - val_acc: 0.7552\n",
      "Epoch 808/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4572 - acc: 0.7812 - val_loss: 0.4867 - val_acc: 0.7552\n",
      "Epoch 809/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4572 - acc: 0.7812 - val_loss: 0.4867 - val_acc: 0.7552\n",
      "Epoch 810/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4572 - acc: 0.7830 - val_loss: 0.4867 - val_acc: 0.7552\n",
      "Epoch 811/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4572 - acc: 0.7812 - val_loss: 0.4867 - val_acc: 0.7552\n",
      "Epoch 812/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4155 - acc: 0.843 - 0s 62us/step - loss: 0.4572 - acc: 0.7812 - val_loss: 0.4867 - val_acc: 0.7552\n",
      "Epoch 813/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4572 - acc: 0.7830 - val_loss: 0.4867 - val_acc: 0.7552\n",
      "Epoch 814/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4572 - acc: 0.7830 - val_loss: 0.4867 - val_acc: 0.7552\n",
      "Epoch 815/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4572 - acc: 0.7830 - val_loss: 0.4867 - val_acc: 0.7552\n",
      "Epoch 816/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4572 - acc: 0.7830 - val_loss: 0.4868 - val_acc: 0.7552\n",
      "Epoch 817/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4572 - acc: 0.7830 - val_loss: 0.4868 - val_acc: 0.7552\n",
      "Epoch 818/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4572 - acc: 0.7812 - val_loss: 0.4868 - val_acc: 0.7552\n",
      "Epoch 819/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4572 - acc: 0.7830 - val_loss: 0.4868 - val_acc: 0.7552\n",
      "Epoch 820/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4572 - acc: 0.7830 - val_loss: 0.4868 - val_acc: 0.7552\n",
      "Epoch 821/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4571 - acc: 0.7830 - val_loss: 0.4868 - val_acc: 0.7552\n",
      "Epoch 822/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4571 - acc: 0.7830 - val_loss: 0.4868 - val_acc: 0.7552\n",
      "Epoch 823/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4571 - acc: 0.7830 - val_loss: 0.4868 - val_acc: 0.7552\n",
      "Epoch 824/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4571 - acc: 0.7830 - val_loss: 0.4868 - val_acc: 0.7552\n",
      "Epoch 825/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4571 - acc: 0.7830 - val_loss: 0.4868 - val_acc: 0.7552\n",
      "Epoch 826/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4571 - acc: 0.7830 - val_loss: 0.4868 - val_acc: 0.7552\n",
      "Epoch 827/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4571 - acc: 0.7830 - val_loss: 0.4868 - val_acc: 0.7552\n",
      "Epoch 828/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4571 - acc: 0.7830 - val_loss: 0.4868 - val_acc: 0.7552\n",
      "Epoch 829/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4571 - acc: 0.7830 - val_loss: 0.4868 - val_acc: 0.7552\n",
      "Epoch 830/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4571 - acc: 0.7830 - val_loss: 0.4868 - val_acc: 0.7552\n",
      "Epoch 831/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4571 - acc: 0.7830 - val_loss: 0.4868 - val_acc: 0.7552\n",
      "Epoch 832/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4571 - acc: 0.7830 - val_loss: 0.4868 - val_acc: 0.7552\n",
      "Epoch 833/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4571 - acc: 0.7830 - val_loss: 0.4868 - val_acc: 0.7552\n",
      "Epoch 834/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 57us/step - loss: 0.4571 - acc: 0.7830 - val_loss: 0.4868 - val_acc: 0.7552\n",
      "Epoch 835/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4571 - acc: 0.7830 - val_loss: 0.4868 - val_acc: 0.7552\n",
      "Epoch 836/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4571 - acc: 0.7830 - val_loss: 0.4868 - val_acc: 0.7552\n",
      "Epoch 837/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4570 - acc: 0.7830 - val_loss: 0.4868 - val_acc: 0.7552\n",
      "Epoch 838/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4570 - acc: 0.7830 - val_loss: 0.4868 - val_acc: 0.7552\n",
      "Epoch 839/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4570 - acc: 0.7830 - val_loss: 0.4868 - val_acc: 0.7552\n",
      "Epoch 840/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4570 - acc: 0.7830 - val_loss: 0.4868 - val_acc: 0.7552\n",
      "Epoch 841/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4570 - acc: 0.7830 - val_loss: 0.4868 - val_acc: 0.7552\n",
      "Epoch 842/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4570 - acc: 0.7830 - val_loss: 0.4869 - val_acc: 0.7552\n",
      "Epoch 843/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4570 - acc: 0.7830 - val_loss: 0.4869 - val_acc: 0.7552\n",
      "Epoch 844/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4570 - acc: 0.7830 - val_loss: 0.4869 - val_acc: 0.7552\n",
      "Epoch 845/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4570 - acc: 0.7830 - val_loss: 0.4869 - val_acc: 0.7552\n",
      "Epoch 846/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4570 - acc: 0.7830 - val_loss: 0.4869 - val_acc: 0.7552\n",
      "Epoch 847/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4570 - acc: 0.7830 - val_loss: 0.4869 - val_acc: 0.7552\n",
      "Epoch 848/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4570 - acc: 0.7830 - val_loss: 0.4869 - val_acc: 0.7552\n",
      "Epoch 849/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4570 - acc: 0.7830 - val_loss: 0.4869 - val_acc: 0.7552\n",
      "Epoch 850/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4569 - acc: 0.7830 - val_loss: 0.4869 - val_acc: 0.7552\n",
      "Epoch 851/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4569 - acc: 0.7830 - val_loss: 0.4869 - val_acc: 0.7552\n",
      "Epoch 852/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4569 - acc: 0.7830 - val_loss: 0.4869 - val_acc: 0.7552\n",
      "Epoch 853/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4569 - acc: 0.7830 - val_loss: 0.4869 - val_acc: 0.7552\n",
      "Epoch 854/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4569 - acc: 0.7830 - val_loss: 0.4869 - val_acc: 0.7552\n",
      "Epoch 855/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4569 - acc: 0.7830 - val_loss: 0.4869 - val_acc: 0.7552\n",
      "Epoch 856/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4569 - acc: 0.7830 - val_loss: 0.4869 - val_acc: 0.7552\n",
      "Epoch 857/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4569 - acc: 0.7830 - val_loss: 0.4869 - val_acc: 0.7552\n",
      "Epoch 858/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4569 - acc: 0.7830 - val_loss: 0.4869 - val_acc: 0.7552\n",
      "Epoch 859/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4569 - acc: 0.7830 - val_loss: 0.4869 - val_acc: 0.7552\n",
      "Epoch 860/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4569 - acc: 0.7830 - val_loss: 0.4869 - val_acc: 0.7552\n",
      "Epoch 861/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4569 - acc: 0.7830 - val_loss: 0.4869 - val_acc: 0.7552\n",
      "Epoch 862/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4569 - acc: 0.7830 - val_loss: 0.4869 - val_acc: 0.7552\n",
      "Epoch 863/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4569 - acc: 0.7830 - val_loss: 0.4869 - val_acc: 0.7552\n",
      "Epoch 864/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4568 - acc: 0.7830 - val_loss: 0.4869 - val_acc: 0.7552\n",
      "Epoch 865/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4569 - acc: 0.7830 - val_loss: 0.4869 - val_acc: 0.7552\n",
      "Epoch 866/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4569 - acc: 0.7830 - val_loss: 0.4869 - val_acc: 0.7552\n",
      "Epoch 867/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4568 - acc: 0.7830 - val_loss: 0.4869 - val_acc: 0.7552\n",
      "Epoch 868/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4568 - acc: 0.7830 - val_loss: 0.4870 - val_acc: 0.7552\n",
      "Epoch 869/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4568 - acc: 0.7830 - val_loss: 0.4870 - val_acc: 0.7552\n",
      "Epoch 870/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4568 - acc: 0.7830 - val_loss: 0.4870 - val_acc: 0.7552\n",
      "Epoch 871/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4568 - acc: 0.7830 - val_loss: 0.4870 - val_acc: 0.7552\n",
      "Epoch 872/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4568 - acc: 0.7830 - val_loss: 0.4870 - val_acc: 0.7552\n",
      "Epoch 873/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4568 - acc: 0.7830 - val_loss: 0.4870 - val_acc: 0.7552\n",
      "Epoch 874/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4568 - acc: 0.7830 - val_loss: 0.4870 - val_acc: 0.7552\n",
      "Epoch 875/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4568 - acc: 0.7830 - val_loss: 0.4870 - val_acc: 0.7552\n",
      "Epoch 876/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4568 - acc: 0.7830 - val_loss: 0.4870 - val_acc: 0.7552\n",
      "Epoch 877/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4568 - acc: 0.7830 - val_loss: 0.4870 - val_acc: 0.7552\n",
      "Epoch 878/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4568 - acc: 0.7830 - val_loss: 0.4870 - val_acc: 0.7552\n",
      "Epoch 879/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4568 - acc: 0.7830 - val_loss: 0.4870 - val_acc: 0.7552\n",
      "Epoch 880/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4568 - acc: 0.7830 - val_loss: 0.4870 - val_acc: 0.7552\n",
      "Epoch 881/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4567 - acc: 0.7830 - val_loss: 0.4870 - val_acc: 0.7552\n",
      "Epoch 882/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4567 - acc: 0.7830 - val_loss: 0.4870 - val_acc: 0.7552\n",
      "Epoch 883/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4567 - acc: 0.7830 - val_loss: 0.4870 - val_acc: 0.7552\n",
      "Epoch 884/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4567 - acc: 0.7830 - val_loss: 0.4870 - val_acc: 0.7552\n",
      "Epoch 885/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4567 - acc: 0.7847 - val_loss: 0.4870 - val_acc: 0.7552\n",
      "Epoch 886/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4567 - acc: 0.7830 - val_loss: 0.4870 - val_acc: 0.7552\n",
      "Epoch 887/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4567 - acc: 0.7830 - val_loss: 0.4870 - val_acc: 0.7552\n",
      "Epoch 888/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4567 - acc: 0.7830 - val_loss: 0.4870 - val_acc: 0.7552\n",
      "Epoch 889/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4567 - acc: 0.7830 - val_loss: 0.4870 - val_acc: 0.7552\n",
      "Epoch 890/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4567 - acc: 0.7830 - val_loss: 0.4870 - val_acc: 0.7552\n",
      "Epoch 891/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4567 - acc: 0.7847 - val_loss: 0.4870 - val_acc: 0.7552\n",
      "Epoch 892/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4567 - acc: 0.7847 - val_loss: 0.4870 - val_acc: 0.7552\n",
      "Epoch 893/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4567 - acc: 0.7830 - val_loss: 0.4870 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 894/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4567 - acc: 0.7830 - val_loss: 0.4871 - val_acc: 0.7552\n",
      "Epoch 895/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4567 - acc: 0.7830 - val_loss: 0.4871 - val_acc: 0.7552\n",
      "Epoch 896/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4567 - acc: 0.7830 - val_loss: 0.4871 - val_acc: 0.7552\n",
      "Epoch 897/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4566 - acc: 0.7847 - val_loss: 0.4871 - val_acc: 0.7552\n",
      "Epoch 898/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4566 - acc: 0.7847 - val_loss: 0.4871 - val_acc: 0.7552\n",
      "Epoch 899/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4566 - acc: 0.7847 - val_loss: 0.4871 - val_acc: 0.7552\n",
      "Epoch 900/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4566 - acc: 0.7847 - val_loss: 0.4871 - val_acc: 0.7552\n",
      "Epoch 901/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4566 - acc: 0.7830 - val_loss: 0.4871 - val_acc: 0.7552\n",
      "Epoch 902/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4566 - acc: 0.7847 - val_loss: 0.4871 - val_acc: 0.7552\n",
      "Epoch 903/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4566 - acc: 0.7830 - val_loss: 0.4871 - val_acc: 0.7552\n",
      "Epoch 904/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4566 - acc: 0.7847 - val_loss: 0.4871 - val_acc: 0.7552\n",
      "Epoch 905/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4566 - acc: 0.7847 - val_loss: 0.4871 - val_acc: 0.7552\n",
      "Epoch 906/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4566 - acc: 0.7847 - val_loss: 0.4871 - val_acc: 0.7552\n",
      "Epoch 907/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4566 - acc: 0.7847 - val_loss: 0.4871 - val_acc: 0.7552\n",
      "Epoch 908/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4566 - acc: 0.7847 - val_loss: 0.4871 - val_acc: 0.7552\n",
      "Epoch 909/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4566 - acc: 0.7847 - val_loss: 0.4871 - val_acc: 0.7552\n",
      "Epoch 910/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4566 - acc: 0.7847 - val_loss: 0.4871 - val_acc: 0.7552\n",
      "Epoch 911/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4566 - acc: 0.7847 - val_loss: 0.4871 - val_acc: 0.7552\n",
      "Epoch 912/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.7237 - acc: 0.656 - 0s 69us/step - loss: 0.4566 - acc: 0.7847 - val_loss: 0.4871 - val_acc: 0.7552\n",
      "Epoch 913/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4565 - acc: 0.7847 - val_loss: 0.4871 - val_acc: 0.7552\n",
      "Epoch 914/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4565 - acc: 0.7847 - val_loss: 0.4871 - val_acc: 0.7552\n",
      "Epoch 915/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4565 - acc: 0.7847 - val_loss: 0.4871 - val_acc: 0.7552\n",
      "Epoch 916/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4565 - acc: 0.7847 - val_loss: 0.4871 - val_acc: 0.7552\n",
      "Epoch 917/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4565 - acc: 0.7847 - val_loss: 0.4871 - val_acc: 0.7552\n",
      "Epoch 918/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4565 - acc: 0.7847 - val_loss: 0.4871 - val_acc: 0.7552\n",
      "Epoch 919/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4565 - acc: 0.7847 - val_loss: 0.4871 - val_acc: 0.7552\n",
      "Epoch 920/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4565 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7552\n",
      "Epoch 921/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4565 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7552\n",
      "Epoch 922/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4565 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7552\n",
      "Epoch 923/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4565 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7552\n",
      "Epoch 924/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4565 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7552\n",
      "Epoch 925/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4565 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7552\n",
      "Epoch 926/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4565 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7552\n",
      "Epoch 927/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4565 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7552\n",
      "Epoch 928/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4565 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7552\n",
      "Epoch 929/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4565 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7552\n",
      "Epoch 930/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4564 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7552\n",
      "Epoch 931/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4564 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7552\n",
      "Epoch 932/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4564 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7552\n",
      "Epoch 933/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4564 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7552\n",
      "Epoch 934/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4564 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7552\n",
      "Epoch 935/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4564 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7552\n",
      "Epoch 936/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4564 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7552\n",
      "Epoch 937/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4564 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7552\n",
      "Epoch 938/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4564 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7552\n",
      "Epoch 939/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4564 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7552\n",
      "Epoch 940/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4564 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7552\n",
      "Epoch 941/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4564 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7552\n",
      "Epoch 942/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4564 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7552\n",
      "Epoch 943/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4564 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7552\n",
      "Epoch 944/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4564 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7552\n",
      "Epoch 945/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4564 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7552\n",
      "Epoch 946/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4564 - acc: 0.7847 - val_loss: 0.4872 - val_acc: 0.7552\n",
      "Epoch 947/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4564 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7552\n",
      "Epoch 948/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4563 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7552\n",
      "Epoch 949/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4563 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7552\n",
      "Epoch 950/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4563 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7552\n",
      "Epoch 951/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4563 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7552\n",
      "Epoch 952/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4563 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7552\n",
      "Epoch 953/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 61us/step - loss: 0.4563 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7552\n",
      "Epoch 954/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4563 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7552\n",
      "Epoch 955/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4563 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7552\n",
      "Epoch 956/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4563 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7552\n",
      "Epoch 957/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4563 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7552\n",
      "Epoch 958/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4563 - acc: 0.7830 - val_loss: 0.4873 - val_acc: 0.7552\n",
      "Epoch 959/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4563 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7552\n",
      "Epoch 960/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4563 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7552\n",
      "Epoch 961/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4563 - acc: 0.7830 - val_loss: 0.4873 - val_acc: 0.7552\n",
      "Epoch 962/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4563 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7552\n",
      "Epoch 963/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4562 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7552\n",
      "Epoch 964/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4562 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7604\n",
      "Epoch 965/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4562 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7656\n",
      "Epoch 966/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4563 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7656\n",
      "Epoch 967/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4562 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7656\n",
      "Epoch 968/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4562 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7656\n",
      "Epoch 969/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4562 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7656\n",
      "Epoch 970/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4562 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7656\n",
      "Epoch 971/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4562 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7656\n",
      "Epoch 972/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4562 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7656\n",
      "Epoch 973/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4562 - acc: 0.7847 - val_loss: 0.4873 - val_acc: 0.7656\n",
      "Epoch 974/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4562 - acc: 0.7847 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 975/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4562 - acc: 0.7847 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 976/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4562 - acc: 0.7847 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 977/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4562 - acc: 0.7847 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 978/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4562 - acc: 0.7847 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 979/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4562 - acc: 0.7847 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 980/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4562 - acc: 0.7830 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 981/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4562 - acc: 0.7847 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 982/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4561 - acc: 0.7847 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 983/1000\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4561 - acc: 0.7830 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 984/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4561 - acc: 0.7847 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 985/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4561 - acc: 0.7847 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 986/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4561 - acc: 0.7830 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 987/1000\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4561 - acc: 0.7847 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 988/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4561 - acc: 0.7830 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 989/1000\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4561 - acc: 0.7847 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 990/1000\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.4561 - acc: 0.7830 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 991/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4561 - acc: 0.7847 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 992/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4561 - acc: 0.7847 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 993/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4561 - acc: 0.7847 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 994/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4561 - acc: 0.7830 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 995/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4561 - acc: 0.7847 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 996/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4561 - acc: 0.7847 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 997/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4561 - acc: 0.7847 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 998/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4560 - acc: 0.7830 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 999/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4561 - acc: 0.7830 - val_loss: 0.4874 - val_acc: 0.7656\n",
      "Epoch 1000/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4560 - acc: 0.7830 - val_loss: 0.4874 - val_acc: 0.7656\n"
     ]
    }
   ],
   "source": [
    "## Note that when we call \"fit\" again, it picks up where it left off\n",
    "run_hist_1b = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x28389fee320>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6sAAAHVCAYAAAAXVW0dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28389f96438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = len(run_hist_1.history[\"loss\"])\n",
    "m = len(run_hist_1b.history['loss'])\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"loss\"], 'hotpink', marker='.', label=\"Train Loss - Run 2\")\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"val_loss\"], 'LightSkyBlue', marker='.',  label=\"Validation Loss - Run 2\")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this graph begins where the other left off.  While the training loss is still going down, it looks like the validation loss has stabilized (or even gotten worse!).  This suggests that our network will not benefit from further training.  What is the appropriate number of epochs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next task\n",
    "Do the following in the cells below:\n",
    "- Build a model with two hidden layers, each with 6 nodes\n",
    "- Use the \"relu\" activation function for the hidden layers, and \"sigmoid\" for the final layer\n",
    "- Use a learning rate of .003 and train for 1500 epochs\n",
    "- Graph the trajectory of the loss functions, accuracy on both train and test set\n",
    "- Plot the roc curve for the predictions\n",
    "\n",
    "Experiment with different learning rates, numbers of epochs, and network structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Type your code here to with layer 1,2 having activation relu and layer 3 with activation sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Type your code here to plot the loss accuracy and ROC curve"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
